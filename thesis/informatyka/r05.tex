\chapter{System do zarz±dzania wielokomputerowym serwerem WWW}
\label{r05}

\section{Wstêp}
W tym rozdziale zostanie przedstawiony opis konfiguracji stanowiska u¿ytego do badañ nad charakterystykami 
ró¿nych konfiguracji i algorytmów klastra serwerów WWW zestawionego w ¶rodowisku systemu AIX i Windows NT z 
wykorzystaniem pakietu IBM WebSphere Performance Pack. Opisana zostanie konfiguracja sieci komputerowej, w której zosta³y 
przeprowadzone badania oraz charakterystyka poszczególnych testów.

\section{Charakterystyka u¿ytego oprogramowania}

\subsection{IBM WebSphere Performance Pack}

IBM WebSphere Performance Pack jest oprogramowaniem infrastrukturalnym WWW wi±¿±cym ze sob± skalowalno¶æ, niezawodno¶æ i 
wydajno¶æ, które to cechy s± niezbêdne dla aplikacji e-biznesu zarówno w ¶rodowiskach lokalnych jak i rozproszonych. Jego 
funkcje ³±cz± ze sob± znakomity caching, zarz±dzanie plikami i równowa¿enie obci±¿enia, które razem kompensuj± wrodzone 
s³abo¶ci Internetu by wspieraæ krytyczne aplikacje biznesowe.

IBM WebSphere Performance Pack sk³ada siê z trzech g³ównych elementów, które to pozwalaj± zredukowaæ obci±¿enie serwera WWW, 
zwiêkszyæ dyspozycyjno¶æ zasobów (zawarto¶ci) i zwiêkszyæ wydajno¶æ serwera WWW \cite{GettingStarted}:
\begin{description}
\item[Wspó³dzielenie plików]\

Komponent zajmuj±cy siê wspó³dzieleniem plików, znany jako IBM AFS Enterprise File System (AFS), jest systemem plików 
pozwalaj±cym wspó³pracuj±cym hostom (klientom i serwerom) efektywnie wspó³dzieliæ zasoby systemów plików poprzez 
zarówno LAN jak i WAN. Prowadzi on replikacjê informacji pomiêdzy wieloma serwerami w czasie rzeczywistym, gwarantuj±c przy 
tym spójno¶æ danych, dostêpno¶æ, stabilno¶æ i efektywno¶æ w administrowaniu, wymaganych przez du¿e, rozproszone serwisy webowe.
\item[Keszowanie i filtrowanie]\

Komponent odpowiedzialny za pamiêæ podrêczn± i filtrowanie zawarto¶ci webowej, znany jako IBM Web Traffic Express (WTE) jest 
proxy serwerem, który dostarcza wysoce skalowalnych funkcji keszowania i filtrowania zwi±zanych z przesy³aniem ¿±dañ webowych 
i dostarczaniem adresów URL. Modu³ ten jest w stanie zredukowaæ kosztown± szeroko¶æ wykorzystywanego pasma dostêpowego i w 
szybszy sposób, oraz z mniejszymi opó¼nieniami dostarczaæ informacje do klienta.
\item[Równowa¿nie obci±¿enia]\

Modu³ odpowiedzialny za równowa¿enie obci±¿eñ, znany jako IBM SecureWay Network Dispatcher jest serwerem zdolnym do 
dynamicznego monitorowania i równowa¿enia aplikacji i serwerów TCP w czasie rzeczywistym. G³ówn± zalet± tego komponentu jest 
mo¿liwo¶æ dynamicznego zwi±zania ze sob± wielu serwerów TCP tak, ¿e wygl±daj± z sieci jak pojedynczy logicznie serwer.
\end{description}

Ka¿dy z elementów IBM WebSphere Performance Pack mo¿e byæ zainstalowany oddzialenie od pozosta³ych -- tak¿e na innych 
komputerach. Dziêki zwi±zaniu tylu elementów w jednym pakiecie - klient otrzymuje oprogramowanie o scentralizowanej 
administracji i zminimalizowanym koszcie. 

Procedury instalacyjne pozwalaj± wybraæ, który komponent nale¿y zainstalowaæ i na jakich maszynach w sieci maj± siê one 
znajdowaæ. Oprogramowanie to jest portowane na nastêpuj±ce platformy: AIX (od wersji 4.2.1), Solaris (od wersji 2.6) oraz 
MS Windows NT i 2000.

\subsection{Web Traffic Express}
WTE jest naraz kaszuj±cym serwerem proxy i filtrem zawarto¶ci pakietów. Zaawansowane keszowanie pozwala zminimalizowaæ 
wykorzystanie przepustowo¶ci zwiêkszaj±c przy tym pewno¶æ, ¿e klienci spêdz± znacznie mniej czasu podczas pobrania tej samej 
informacji kilka razy \cite{WTEUsersGuide,WTEProgramming}. 

Tradycyjny proxy serwer przesy³a ¿±dania dla URL od klienta i podaje je dalej do serwera przeznaczenia. WTE daje co¶ wiêcej; 
pozwala zapisaæ lub keszowaæ dokumenty, które przesy³a oraz serwowaæ je podczas pó¼niejszych ¿±dañ ze swojego keszu, a nie ze 
¼ród³owego serwera. Co oznacza, ¿e klient ¿±dany dokument otrzymuje szybciej przy zmniejszonym obci±¿eniu ³±cz. 

Modu³ ten posiada tak¿e dodatkowe cechy takie jak:
\begin{itemize}
\item mo¿liwo¶æ utrzymania bardzo du¿ego keszu;
\item opcjê automatycznego od¶wie¿ania tej czê¶ci pamiêci podrêcznej zawieraj±cej najczê¶ciej popobierane strony;
\item mo¿liwo¶æ keszowania tak¿e tych stron, których nag³ówek wymaga by by³y zawsze pobierane ze ¼ród³owego serwera;
\item konfigurowania okresowego porz±dkowania keszu z informacji bezu¿ytecznych w celu poprawy wydajno¶ci i utrzymania 
efektywno¶ci jego dzia³ania;
\item Remote Cache Access (RCA) - w³a¶ciwo¶æ pozwalaj±ca na wielu maszynom z WTE uwspólniania tego samego keszu poprzez 
rozproszony system plików, taki jak AFS, w celu zredukowania redundancji zawarto¶ci.
\end{itemize}

Dodatkowo WTE pozwala na ustawianie filtrowania zawarto¶ci na poziomie serwera proxy -- pozwalaj±c na takie zabiegi jak np. 
blokowanie URL--ów. Zwielokrotnione serwery WTE mog± byæ obci±¿eniowo zrównowa¿one.

Kolejn± cech± jak± posiada ten modu³ jest mo¿liwo¶æ pracy jako przezroczysty proxy -- czyli taki, do którego dzia³ania nie jest 
potrzebna ¿adna zmiana w konfiguracji przegl±darki klienta -- WTE pracuje wtedy na porcie serwera WWW. 

\subsection{IBM SecureWay Network Dispatcher}
Jest to jedno z pierwszych komercyjnych rozwi±zañ, które pozwala zwiêkszyæ wydajno¶æ i zapewniæ ci±g³± pracê serwisów 
internetowych. Znalaz³o ono ju¿ szerokie zastosowanie w znacz±cych serwisach WWW, takich jak serwisy prowadzone w trakcie 
olimpiad w Atlancie i Nagano. 
System jest przeznaczony do obs³ugi serwerów webowych zbudowanych w technologii klastrowej. Klaster jest grup± serwerów 
webowych obs³uguj±cych jedn± witrynê WWW. Wszystkie serwery w klastrze posiadaj± tê sam± zawarto¶æ i zapytanie mo¿e byæ 
obs³u¿one przez dowolny z nich serwer. Pojedyncze zapytanie obs³ugiwane jest przez jeden z aktualnie sprawnych serwerów. 
IBM SecureWay Network Dispatcher sk³ada siê z trzech komponentów: podsystemu Dispatcher, podsystemu Interactive Session 
Support (ISS) oraz podsystemu Content--based Routing (CBR). Komponenty te mog± byæ u¿ywane ³±cznie lub ka¿dy z 
osobna \cite{LoadBalancingWithND}.

\subsubsection{Komponent Dispatcher}

Podstawowym komponentem systemu jest Dispatcher, którego zadaniem jest zapewnienie równomiernej dystrybucji zapytañ 
realizowanych za po¶rednictwem protoko³u TCP/IP miêdzy wieloma serwerami realizuj±cymi tê sam± us³ugê w konfiguracji 
klastrowej. Funkcjonowanie Network Dispatcher'a opiera siê na architekturze przedstawionego wcze¶niej dystrybutora. 
Dispatcher mo¿e równowa¿yæ obci±¿enia w obrêbie sieci lokalnej lub rozleg³ej. Dla ka¿dego klastra definiuje siê porty, które 
chcemy, aby by³y obs³ugiwane przez klaster, nastêpnie serwery, które bêd± dostarczaæ us³ugi na ka¿dym z zdefiniowanych portów. 
Dispatcher mo¿e obs³ugiwaæ wiele klastrów \cite{NDUsersGuide}. 

Wysok± dostêpno¶æ serwisu osi±gniêto poprzez dzia³anie samego Dispatcher'a, który wykrywa niesprawne serwery w klastrze i 
omija je przy dystrybucji zapytañ oraz poprzez wprowadzenie drugiego systemu Dispatcher'a. W podstawowym trybie pracy 
Dispatcher wymaga, aby wszystkie serwery w klastrze by³y w tej samej podsieci co Dispatcher. Wówczas, aby podwy¿szyæ 
dostêpno¶æ serwisu, mo¿emy skonfigurowaæ drugi system Dispatcher'a, który pe³niæ bêdzie rolê maszyny zapasowej i czuwaj±cej w 
gotowo¶ci do przejêcia zadania równowa¿enia obci±¿enia w przypadku, gdyby maszyna podstawowa Dispatcher'a przesta³a dzia³aæ 
poprawnie. Mechanizm typu heartbeat zapewnia monitorowanie stanów przez obie maszyny, podstawow± i zapasow± oraz przejêcie 
zadañ w przypadku wyst±pienia awarii.
\begin{figure}[h]
\centering
\includegraphics[width=3in]{./rysunki/dispatcher.eps}
\caption{Konfiguracja logiczna modu³u dispatcher}
\label{dispatcher}
\end{figure}

Dispatcher realizuje swoje zadania za pomoc± trzech wewnêtrznych komponentów: Egzekutora, Menad¿era i Doradców. Egzekutor 
realizuje równowa¿enie obci±¿eñ. Dla pakietu wysy³anego w ramach nowego po³±czenia miêdzy klientem a serwerem WEB Dispatcher 
sprawdza, który z serwerów mo¿e przej±æ obs³ugê zlecenia na ¿±danym przez klienta porcie i adresie klastra. Nastêpnie, dla 
ka¿dego takiego serwera, na podstawie zgromadzonych wag okre¶laj±cych poziom obci±¿eñ serwerów, okre¶la serwer najmniej 
obci±¿ony, do którego przekazuje pakiet. Je¶li po³±czenie ju¿ istnieje, wtedy bez ¿adnego przetwarzania pakiet jest natychmiast
wysy³any do tego samego serwera, który zosta³ wybrany podczas inicjalizacji po³±czenia. Rozdzia³ zleceñ miêdzy serwery bazuje 
na warto¶ciach wag wskazuj±cych na mo¿liwo¶æ potencjalnego obci±¿enia serwera -- np. je¿eli jeden serwer ma wagê 2, a drugi ma 
wagê 1, to serwer o wadze 2 powinien dostaæ dwa razy wiêcej ¿±dañ ni¿ ten o wadze 1. Wagi mog± byæ ustawiane rêcznie lub przez 
Menad¿era. Najczê¶ciej do ustawiania wag korzysta siê z Menad¿era, który ustawia wagi automatycznie i w sposób adaptacyjny, z 
uwzglêdnieniem aktualnych warunków pracy klastra. Menad¿er mo¿e korzystaæ z wewnêtrznych liczników systemowych oraz z 
informacji dostarczanych przez inne komponenty systemu, takich jak ISS czy WLM. Zarz±dca decyduje który z serwerów jest 
najmniej obci±¿onym poprzez obserwacjê wag serwerów, które to okresowo analizuje i uaktualnia. Decyzjê jak± serwerowi nadaæ 
wagê podejmuje opieraj±c siê na czterech parametrach:
\begin{itemize}
\item liczba aktywnych po³±czeñ realizowanych na ka¿dym serwerze TCP;
\item liczba nowych po³±czeñ na ka¿dym serwerze TCP;
\item danych wej¶ciowych pochodz±cych od doradców;
\item informacji pochodz±cych od narzêdzi monitoruj±cych pracê systemu, takich jak ISS;
\end{itemize}

U¿ywanie zarz±dcy jest opcjonalne, ale je¶li 
nie jest on u¿ywany, równowa¿enie obci±¿enia jest dokonywane przy u¿yciu marszrutowania algorytmem wa¿onego Round Robina, 
gdzie wagi poszczególnych serwerów WWW s± nadawane statycznie; 

Aby daæ administratorowi wiêksz± kontrolê nad tym, gdzie kierowane bêd± ¿±dania ró¿nego typu pochodz±ce od ró¿nych grup 
u¿ytkowników, wprowadzono pojêcie regu³ i grup serwerów im podleg³ych, tj. serwerów, w¶ród których bêdzie realizowane 
równowa¿enie obci±¿enia w razie spe³nienia regu³y. Dla Dispatcher'a dostêpne s± regu³y bazuj±ce na: adresie IP klienta, porcie 
klienta, porze dnia, po³±czeniach na sekundê dla danego portu, aktywnych po³±czeniach dla danego portu. Regu³y daj± mo¿liwo¶æ 
implementacji wysokiej jako¶ci us³ug dla wybranych klientów b±d¼ w okre¶lonej porze dnia. Dostêpna jest tak¿e regu³a ,,zawsze 
prawdziwe'', która oznacza spe³nienie zadanego warunku dla wszystkich przyjmowanych zleceñ.
\begin{figure}[h]
\centering
\includegraphics[width=5in]{./rysunki/executor.eps}
\caption{Architektura Network Dispatcher-a}
\label{dispatcher1}
\end{figure}

Zadaniem doradców jest zbieranie informacji na temat obci±¿enia serwerów. Doradcy sprawdzaj± równie¿, czy serwery, na których 
dokonywane jest równowa¿enie obci±¿eñ, funkcjonuj± prawid³owo. Doradcy okresowo otwieraj± po³±czenia TCP i wysy³aj± do serwera 
wiadomo¶æ z ¿±daniem specyficznym dla danego typu doradcy. Po wys³aniu wiadomo¶ci Doradcy czekaj± na odpowied¼. Po jej 
otrzymaniu wiêkszo¶æ Doradców szacuje warto¶æ obci±¿enia serwera na podstawie czasu, jaki up³yn±³ nim serwer zwróci³ odpowied¼ 
na ¿±danie i zg³aszaj± ten czas (w milisekundach) Menad¿erowi, aby ten na podstawie tej i innych informacji oszacowa³ warto¶æ 
wag poszczególnych serwerów. 

Wraz z produktem IBM SecureWay Network Dispatcher dostarczeni s± doradcy: HTTP, FTP, Telnet, NNTP, POP3, SMTP, SSL, WTE, TCP, 
Ping, WLM. Istnieje równie¿ mo¿liwo¶æ napisania w³asnego doradcy. 

\subsubsection{Komponent Interactive Session Support}

Interactive Session Support (ISS) jest komponentem wspó³pracuj±cym z Domain Name Server (DNS) w jednym z trzech mo¿liwych 
trybów pracy: DNS--Update, DNS--Replace lub DNS--Ignore. Komponent ten mo¿e byæ równie¿ wykorzystany do zbierania informacji na 
temat obci±¿enia serwerów, któr± nastêpnie przekazuje do Dispatcher'a. W trybie DNS--Update ISS uaktualnia serwer nazw DNS. W 
tym trybie dzia³a on w po³±czeniu z serwerem nazw w celu mapowania nazw DNS na adresy IP serwerów najlepiej nadaj±cych siê do 
obs³ugi danego zadania. W trybie DNS--Replace ISS spe³nia dla ograniczonej podgrupy sieci funkcjê serwera nazw, nie zawiera 
jednak wszystkich funkcji standardowego DNS. W trybie DNS--Ignore, ISS dzia³a w po³±czeniu z Dispatcher'em w celu lepszego 
równowa¿enia obci±¿eñ.
\begin{figure}[h]
\centering
\includegraphics[width=3in]{./rysunki/ISS.eps}
\caption{Konfiguracja logiczna modu³u Interactive Session Support}
\label{ISS}
\end{figure}

Obci±¿enie na poszczególnych wêz³ach mo¿e byæ okre¶lane poprzez pomiar wykorzystania ró¿nych zasobów, takich jak ilo¶æ wolnej 
pamiêci, u¿ycie procesora czy licznik procesów. ISS pos³uguje siê jedn± z trzech strategii alokacji zleceñ: RoundRobin, Best, 
i Statistical RoundRobin.

Przy u¿yciu algorytmu RoundRobin, ISS równowa¿y obci±¿enia serwerów w sposób klasyczny dla tej strategii. Z danej grupy 
serwerów ISS wybiera na okre¶lony przedzia³ czasu jeden serwer, który bêdzie obci±¿any w tym przedziale czasu. ISS nie zwraca 
wówczas uwagi na poziom obci±¿enia serwera, ale te¿ nie zaleci serwera, który niedomaga. Takie podej¶cie mo¿e funkcjonowaæ 
dobrze, pod warunkiem ¿e obci±¿enie wywo³ywane przez klientów jest równomierne. Zalet± tej metody jest brak obci±¿enia 
serwerów poprzez procesy pomiarowe. Natomiast jej du¿± wad± jest brak elastyczno¶ci. Metoda ta mo¿e powodowaæ, ¿e np. serwer, 
który ju¿ jest bardzo obci±¿ony, mo¿e byæ dalej preferowany przez ISS, gdy¿ nie skoñczy³ siê jeszcze przydzielony mu czas.
Stosuj±c strategiê Best, w czasie trwania okre¶lonego interwa³u, ISS trasuje ¿±dania do serwera, który na pocz±tku tego 
interwa³u mia³ najni¿szy poziom obci±¿enia. Tak jak w przypadku poprzedniej metody, korzystaj±cy z niej ISS nie zaleci 
serwera, który przesta³ funkcjonowaæ. Ta metoda selekcji sprawdza siê bardzo dobrze dla ka¿dego czasu trwania po³±czeñ, pod 
warunkiem, ¿e czêsto¶æ nowych po³±czeñ bêdzie relatywnie niska w stosunku do ustawionego interwa³u czasu przydzia³u dla 
pojedynczego serwera. Ta strategia jest przyjmowana domy¶lnie przez system.
W strategii Statistical RoundRobin, ISS równowa¿y obci±¿enia na podstawie statystyk obci±¿enia, które generuje dla wszystkich 
serwerów. Wykorzystuje te statystyki do budowy profilu najbardziej i najmniej obci±¿onych serwerów, a nastêpnie w ci±gu 
trwania interwa³u ,,heartbeat'' rozdziela ¿±dania pomiêdzy serwery, proporcjonalnie do ich obci±¿enia. Równie¿ przy u¿yciu tej 
metody ISS nie zaleci niedomagaj±cego serwera. Metoda ta sprawdza siê wszêdzie tam, gdzie wystêpuje du¿a czêsto¶æ 
krótkotrwa³ych po³±czeñ.

\subsubsection{Komponent Content Base Routing}
 
Komponent CBR mo¿e byæ skonfigurowany z WTE (Web Traffic Express) dla serwerów HTTP, lub jako CBR proxy (bez wspó³pracy z WTE)
dla serwerów IMAP i POP3. CBR wspó³pracuje wespó³ z Web Traffic Express, w ten sposób, ¿e klient wysy³a ¿±danie do WTE, który 
jest skonfigurowany by móc korzystaæ z CBR; CBR musi byæ zainstalowany na tej samej maszynie co serwer WTE. WTE wypytuje 
komponent CBR który serwer ma obs³u¿yæ ¿±danie. Gdy CBR otrzyma ¿±danie próbuje je dopasowaæ do ustawionych regu³. Je¶li 
pasuj±, CBR wybiera serwer, z grupy skonfigurowanych do odbierania konkretnego typu ¿±dañ, jednocze¶nie równowa¿±c pomiêdzy 
t± grup± serwerów obci±¿enie. 

CBR jest bardzo podobny w strukturze do pakietu Dispatcher. Trzy kluczowe elementy CBR (Egzekutor, Menad¿er i Doradcy) 
wspó³pracuj± by zrównowa¿yæ i rozdzieliæ przychodz±ce ¿±dania pomiêdzy serwery w zbli¿ony sposób jak w Dispatcherze. 
Jednak¿e w przeciwieñstwie do modu³u Dispatcher -- CBR nie oferuje funkcji zwiêkszonej dostêpno¶ci. Jednak¿e mo¿na po³±czyæ 
kilka serwerów CBR i równowa¿yæ ich obci±¿enie poprzez serwer ND, który to mo¿e sprawdzaæ czy ¿aden z CBR serwerów nie 
przesta³ odpowiadaæ.

\begin{itemize}
\item CBR z WTE (obs³uguj±cy ruch HTTP)

Komponent CBR wspó³pracuj±c z Web Traffic Express dzia³a jako proxy serwer przekierowuj±c rz±dania klientów do odpowiednich
serwerów. Modu³ WTE jest proxy serwerem, pozwalaj±cym manipulowaæ pamiêci± podrêczn± w celu szybkiego transferu dokumentów
przy niewielkich wymaganiach dotycz±cych przepustowo¶ci sieci. CBR za¶ jest w stanie przefiltrowaæ zawarto¶æ stron WWW 
korzystaj±c ze specyficznych ci±gów warunków nazywanych rolami. CBR daje mo¿liwo¶æ okre¶lenia grupy serwerów, które powinny
przej±æ ¿±danie opieraj±c siê na wyra¿eniach regularnych pasuj±cych do zawarto¶ci ¿±dania. Poniewa¿ CBR pozwala na
,,przywi±zanie'' poszczególnych serwerów do ka¿dego typu ¿±dania nastêpuje zrównowa¿enie obci±¿enia serwerów. CBR potrafi
tak¿e wykrywaæ kiedy serwer przestaje odpowiadaæ wy³±czaj±c routing ¿±dañ do niego. Zastosowane w module CBR algorytmy
równowa¿enia obci±¿enia s± identyczny z tymi zastosowanymi w komponencie Dispatcher. 

Sposób dzia³ania CBR: gdy klienckie ¿±danie jest przesy³ane do WTE proxy nastêpuje tu korelowanie zawarto¶ci ¿±dania
z regu³ami zdefiniowanymi w module CBR. Je¶li zbiór regu³ pasuje do zawarto¶ci ¿±dania jeden z serwerów ,,zwi±zanych'' z 
obs³ug± tego typu ¿±dañ zostaje wybrany do przyjêcia ¿±dania. Wtedy WTE jako proxy serwer przekierowuje do wybranego serwera 
¿±danie. Jak widaæ modu³ WTE musi byæ uruchomiony zanim CBR zostanie skonfigurowany, poniewa¿ dzia³a on jako podproces WTE.
Oczywi¶cie oba modu³y musz± znajdowaæ siê na tej samej maszynie.

Oznacza to podzia³ farmy serwerów na czê¶ci realizuj±ce odpowiedni typ ¿±dañ. Taki podzia³ jest przezroczysty dla klienta. 
Mo¿na np. podzieliæ witrynê na dwie czê¶ci -- kilka serwerów realizuj±cych tylko ¿±dania CGI, a pozosta³e resztê ruchu HTTP.
Pozwala to na realizacjê intensywnie przetwarzanych skryptów CGI w sposób nie koliduj±cy z prac± reszty serwerów WWW 
obs³uguj±cych normalny ruch HTTP -- co oznacza dla klienta lepszy ogólny czas odpowiedzi. W ten sposób komputery o wiêkszej
mocy obliczeniowej mo¿na przeznaczyæ na obs³uge normalnego ruchu bez kosztownego upgradu wszystkich komputerów.
\begin{figure}[h]
\centering
\includegraphics[width=3in]{./rysunki/CBR-WTE.eps}
\caption{Konfiguracja logiczna modu³u Content Base Routing (wraz z WTE)}
\label{CBR}
\end{figure}


Inn± mo¿liwo¶ci± podzia³u serwera WWW jest bezpo¶rednie przekierowanie klientów, którzy chc± dostaæ siê do stron wymagaj±cych
rejestracji, do jednego typu serwerów, a resztê do pozosta³ych. Wtedy obs³ug± stron wymagaj±cych rejestracji zajmuj± siê
np. komputery o wiêkszej mocy obliczeniowej i klienci, którzy s± ju¿ zarejestrowani maj± lepszy czas odpowiedzi ni¿ inni. 

Wybieraj±c role dokonuj±ce równowa¿enia obci±¿enia nale¿y sprawdziæ czy wszystkie ¿±dania do klastra WWW posiadaj± role, 
na podstawie ktorych mo¿na wybraæ serwer. Je¶li tak nie jest, tzn. je¶li przychodz±ce ¿±danie nie pasuje do ¿adnej roli, klient
otrzyma komunikat o b³êdzie pochodz±cy z WTE. Najprostszym rozwi±zaniem jest stworzenie roli zawsze prawdziwej z bardzo 
wysokim priorytetem i ,,przywi±zanie'' jej do osobnej grupy serwerów.

\item CBR proxy (obs³uguj±cy ruch IMAP i POP3)

CBR bez WTE mo¿e byæ proxy wybieraj±cym odpowiedni serwer opieraj±c siê na ID u¿ytkownika i ha¶le. Nie wspiera wtedy opartego
na rolach równowa¿enia obci±¿eñ.
\end{itemize}

\subsection{Content Base Routing -- konfiguracja}

CBR mo¿e zostaæ skonfigurowany jako proxy dla us³ug opartych o protoko³y POP3 lub IMAP -- dzia³a wtedy bez wspó³pracy
z modu³em WTE, lub tez jako narzêdzie równowa¿±ce obci±¿enie ruchu HTTP -- systemem proxy jest wtedy WTE  \cite{NDUsersGuide}.

Modu³ ten mo¿na skonfigurowaæ za pomoc± linii komend lub z pomoc± graficznego interfejsu u¿ytkownika
(\emph{GUI}). Modu³ Content Base Routing jest bardzo podobny w architekturze do Dispatchera. Sk³ada siê on z trzech
funkcjonalnych czê¶ci:
    \begin{itemize}
    \item Egzekutor -- zajmuje siê równowa¿eniem obci±¿enia zapytañ klienckich. Jest zawsze uruchomiany je¶li CBR
    jest w u¿yciu;
    \item Menad¿er -- ustala wagi dla poszczególnych serwerów opieraj±c siê na:
        \begin{itemize}
        \item wewnêtrznych licznikach Egzekutora;
        \item informacjach zbieranych z serwerów przez doradców;
        \item informacjach z programów monitoruj±cych prace systemu, takich jak ISS czy WLM.
        \end{itemize}
        Korzystanie z menad¿era jest opcjonalne. Jednak¿e gdy jest on nieu¿ywany -- równowa¿enie obci±¿enia jest
        realizowane za pomoc± algorytmu statycznego Wa¿onego Round--Robin opartego na domy¶lnych wagach, a
        doradcy nie bêd± wtedy dostêpni.
    \item Doradcy -- wypytuj± serwery oraz analizuj± dane o ich stanie, by nastêpnie z tak spreparowanych danych
    Menad¿er ustali³ odpowiednie wagi. Korzystanie z Doradców jest równie¿ opcjonalne, a typowej konfiguracji wrêcz
    mo¿e nie byæ potrzebne, jednak¿e mimo to poleca siê z nich korzystaæ.
    \end{itemize}

Wszystkie trzy czê¶ci (Egzekutor, Menad¿er oraz Doradcy) wspó³pracuj± ze sob± w celu rozbalansowania i
rozpropagowania przychodz±cych ¿±dañ pomiêdzy serwery.

Aby skonfigurowaæ wstêpnie CBR, mo¿na pos³u¿yæ siê jedna z czterech dostêpnych metod:
    \begin{enumerate}
    \item Linii komend;
    \item Skryptów konfiguracyjnych;
    \item Graficznego interfejsu u¿ytkownika;
    \item Konfiguracyjnego wizarda.
    \end{enumerate}

\subsubsection{Konfiguracja maszyny CBR}

Aby moc konfigurowaæ CBR, nale¿y mieæ status root--a w systemie. Nale¿y tak¿e znaæ adresy ka¿dego z
konfigurowanych klastrow serwerów. Pojedynczy adres IP jest tu u¿ywany jako jeden adres dla ca³ego klastra.
Zanim jednak¿e przeprowadzi siê konfiguracje i uruchomienie modu³u CBR -- musi byæ ju¿ skonfigurowany i
uruchomiony WTE. Konfiguracje wykonuje siê w takiej kolejno¶ci (konfiguracja dotyczy tylko ruchu HTTP):
    \begin{enumerate}
    \item Uruchomienie WTE
    \item Po uruchomieniu WTE, nale¿y zdefiniowaæ klaster WWW, oraz ustawiæ specyficzne dla tego
    klastra opcje. Do zdefiniowania klastra oraz opcji s³u¿y komenda:\\
    
    cbrcontrol cluster set \emph{cluster opcja warto¶æ}\\
    
    \item Nastêpnym krokiem jest zdefiniowanie portów ustalenie ich opcji. Wykonuje siê to komend±:\\
    
    cbrcontrol port set \emph{cluster:port opcja warto¶æ}\\
    
    \item Definiowanie poszczególnych komputerów nale¿±cych do serwera:\\
    
    cbrcontrol server add \emph{cluster:port:server}\\
    
    gdzie \emph{server} jest nazw± symboliczn± pojedynczego komputera w klastrze, lub jego adresem IP;
    \item Nastêpnym krokiem jest konfiguracja regu³ CBR. Jest to krok kluczowy w konfiguracji CBR przy ruchu po protokole HTTP
    regu³y (role) definiuj± jak zadanie URL zostanie przes³ane do jednego lub wiêkszej ilo¶ci serwerów. Te specyficzne
    regu³y nazywaj± siê regu³ami zawartosci\footnote{\emph{and. -- content rule}}. Aby zdefiniowaæ regu³ê
    korzysta siê z nastêpuj±cej komendy:\\
    
    cbrcontrol rule add \emph{cluster:port:rule} type content pattern=\emph{pattern}\\
    
    gdzie warto¶æ \emph{pattern} jest wyra¿eniem regularnym, które bêdzie porównywane za ka¿dym razem jak
    nadejdzie ¿±danie od klienta.
    \item Nastêpnie nale¿y dodaæ poszczególne serwery obs³uguj±ce regu³y zawarto¶ci. Gdy wystêpuje dopasowanie
    pomiêdzy ¿±danym URL, a regu³± zawarto¶ci zostaje wybrany najlepszy z serwerów obs³uguj±cych ten typ
    ¿±dañ. Aby wykonaæ takie dopasowanie -- regu³a dopasowania--serwer obs³uguj±cy wykonuje siê polecenie:\\
    
    cbrcontrol rule useserver \emph{cluster:port:rule server}\\
    
    \item Uruchomienie Menad¿era (opcjonalne):\\
    
    cbrcontrol manager start\\

    \item Uruchomienie Doradców (opcjonalne):\\

    cbrcontrol advisor start http \emph{port}

    \item Ostatnim krokiem jest skonfigurowanie Doradców, aby ich informacje bra³y udzia³ w decyzjach równowa¿enia
    obci±¿enia.
    
    \end{enumerate}

\subsubsection{Konfiguracja LB opartego na regu³ach}

Wykorzystuj±c do równowa¿enia obci±¿enia WTE wraz z modu³em CBR -- mo¿na rozdzielaæ ruch sieciowy
wykorzystuj±c nastêpuj±ce typy regu³:
    \begin{itemize}
    \item Adres IP klienta; jest to sytuacja, gdy wymaga siê alokacji zasobów w zale¿no¶ci od tego sk±d przychodzi
    zadanie. Mo¿na za³o¿yæ, ze z pewnych adresów (od klientów) wymagamy by zadania nie by³y realizowane, zatem
    przygotowuje siê regule i nie dodaje do ¿adnego serwera -- wtedy okre¶leni klienci nie bêd± obs³ugiwani
    (wyst±pi b³±d). np.:\\

    ndcontrol rule add 9.67.131.153:80:ni type ip beginrange 9.0.0.0 endrange \
    9.255.255.255\\

    taka regu³a oznacza, ze klienci z domeny IBM--a nie osi±gn± serwera CBR;        
    \item Godzina po³±czenia; tak± regu³ê wykorzystuje siê g³ownie przy okre¶lonych planach obs³ugi obci±¿eñ.
    W przypadku gdy witryna produkcyjna otrzymuje pewna grupê ¿±dañ o pewne dokumenty w okre¶lonym
    (i sta³ym) czasie ka¿dego dnia -- mo¿na tylko dla tych ¿±dañ wyznaczyæ osobne serwery; mo¿e to byæ tak¿e
    przydatne w przypadku gdy w godzinach nocnych (najmniejszy ruch) -- niektóre maszyny z powodu wykonywania
    backupu powinny byæ wy³±czone z realizacji ¿±dañ;
    \item Ilo¶æ po³±czeñ na sekundê, na port; (dzia³a tylko podczas uruchomionego Menad¿era) mo¿na wykorzystywaæ
    np. w przypadku: \emph{if po³±czeñ na sekundê na porcie 80 > 100 then u¿yj te dwa serwery}, \emph{if po³±czeñ
    na sekundê na porcie 80 > 2000 u¿yj te 8 serwerów};
    \item Ca³kowita ilo¶æ aktywnych po³±czeñ na porcie; (wymóg jak wy¿ej -- Menad¿er musi byæ uruchomiony);
    Jest to regu³a potrzebna w przypadku gdy wiadomo kiedy serwer bêdzie np. prze³adowany (przy ilu po³±czeniach)
    Je¶li np. wiadomo, ze serwer nie jest w stanie przyj±æ i zrealizowaæ naraz wiêcej niz. 250 jednoczesnych po³±czeñ
    tworzy siê regu³ê:\\

    ndcontrol rule add 130.40.52.153:80:pool2 type active beginrange 250 endrange 500\\

    Wtedy do tak stworzonej regu³y do pojedynczego serwera mo¿na dodaæ nastêpne maszyny, które w przypadku
    wiêkszej ilo¶ci po³±czeñ zostan± w³±czone w podejmowanie ¿±dañ;
    \item regu³a zawsze prawdziwe; Regu³a ta jest zawsze spe³niona -- chyba ze serwery z którymi jest zwi±zana
    nie pracuj±; przydaj± siê w przypadku gdy nie chcemy aby jakikolwiek klient dosta³ zwrot w postaci b³êdu
    zadania dokumentu (od WTE);
    \item Zawarto¶æ zadania; (jest to regu³a dostêpna tylko z poziomy modu³u CBR); regu³a wykorzystywana
    w przypadku dystrybucji ¿±dañ w zale¿no¶ci od zawarto¶ci zadania; np. gdy potrzeba aby jedna grupa serwerów
    obs³ugiwa³a zadania \emph{cgi--bin}, inna grupa obs³ugiwa³a media strumieniowe (np. real media), za¶ trzecia wszystkie
    pozosta³e wtedy wzorcem pierwszej z regu³ 
    bêdzie ¶cie¿ka do katalogu cgi--bin, drugiej do plików strumieniowych, a trzeciej regu³a zawsze prawdziwe;
    nastêpnie nale¿y tylko przyporz±dkowaæ regu³y do odpowiadaj±cych im grup serwerów;
        \begin{description}
        \item[Typy regu³ dla CBR]\

        sk³adnia wzorców regu³ wygl±da nastêpuj±co: w regule (wzorcu)  nie mo¿e byæ ¿adnych przerw (spacji)
        ani znaków specjalnych:
            \begin{description}
            \item[*] -- odpowiada 0 do x wyst±pienia dowolnych znaków;
            \item[)(] -- u¿ywane do grupowania logicznego;
            \item[\&] -- logiczne AND;
            \item[|] -- logiczne OR;
            \item[!] -- logiczne NOT;
            \end{description}
        Zarezerwowane s³owa (zawsze nastêpuje do nich przyporz±dkowanie w postaci znaku równo¶ci):
            \begin{description}
            \item[client] -- adres IP klienta;
            \item[url] -- URL w zadaniu;
            \item[path] -- sekcja ¶cie¿ka URL--a;
            \item[protocol] -- sekcja protokó³u URL--a;
            \item[refer] -- \emph{quality of service}
            \end{description}
        Poni¿ej znajduj± siê przyk³ady:\\

        \emph{url=http://*/*.gif}\\

        \emph{client=9.32.*}\\

        \emph{!(path=*.jpeg)}\\

        \emph{(path=index/*.gif \& protocol=httpd) | (client=9.1.2.3)}       
        \end{description}
    \end{itemize}

Wszystkie regu³y posiadaj± nazwê, typ, priorytet, pocz±tkowy zakres dzia³ania, koñcowy zakres dzia³ania oraz
grupê obs³uguj±cych serwerów. Dodatkowo, regu³a zawarto¶ci w komponencie CBR posiada zwi±zane ze sob±
wyra¿enie regularne. Regu³y s± wykorzystywane w kolejno¶ci ich priorytetów. Regu³y z niskim priorytetem s±
realizowane wcze¶niej. Innymi s³owy, regu³a z priorytetem 1 jest wykorzystywana przed regu³a o priorytecie 2.
Pierwsza poprawnie dopasowana regu³a zostaje wykorzystana (reszta ju¿ nie jest próbkowana).

Aby regu³a zosta³a spe³niona musza byæ spe³nione naraz dwa warunki:
    \begin{enumerate}
    \item Predykat regu³y musi byæ prawdziwy. Co oznacza, ze warto¶æ porównana musi znajdowaæ siê pomiêdzy
    warto¶ciami pocz±tkowa i koñcowa, lub zawarto¶æ zadania musi pasowaæ do wyra¿enia regularnego
    wyspecyfikowanego w wzorcu\footnote{ang. \emph{pattern}}. Dla regu³ o typie ,,zawsze prawdziwy''
    regu³a jest zawsze spe³niona (bez warunków);
    \item Je¶li istniej± serwery zwi±zane do poszczególnych regu³, co najmniej jeden z nich musi byæ dostêpny do
    odebranie zadania.
    \end{enumerate}

W przypadku, gdy nie ma serwerów dla których konkretna regu³a nie mog³aby byæ spe³niona -- zadanie zostaje
porzucone, a CBR zwróci do serwera proxy (WTE) komunikat b³êdu. Je¶li za¶ ¿adna z regu³ nie mo¿e zostaæ
spe³niona -- jak wy¿ej -- CBR zwróci do WTE komunikat b³êdu.

\subsection{Narzêdzie do testów -- Astra Load Runner}

Narzêdziem testowym w wykonanym projekcie by³o oprogramowanie firmy Mercury Interactive\footnote{http://www.merc-int.com}
-- Astra Load Runner. Jego wybór by³ spowodowany szerokim wachlarzem mo¿liwo¶ci testowych, obs³ugiwanych protoko³ów oraz
mo¿liwo¶ci dokonywania analiz i wizualizacji rezultatów. Poni¿ej znajduje siê krótka charakterystyka tego progrmau:

LoadRunner jest narzêdziem do testów wydajno¶ciowych, dziêki którym mo¿na sprawdziæ wydajno¶æ i skalowalno¶æ systemu webowego.
Jest on w stanie generowaæ dziesi±tki, setki czy nawet tysi±ce jednoczesnych u¿ytkowników, umo¿liwiaj±c w ten sposób wykrycie 
wszystkich s³abych, b±d¼ niewydajnych miejsc w naszym systemie. W czasie trwania testu dostêpne s± ró¿norakie 
monitory, wy¶wietlaj±ce ca³y czas wszystkie interesuj±ce parametry systemu funkcjonuj±cego pod obci±¿eniem. Dziêki 
tym wbudowanym monitorom mo¿na np.:  na bie¿±co ¶ledziæ czasy odpowiedzi wszystkich serwerów, czasy wykonywania siê danych 
transakcji czy szybko¶æ transferu danych w sieci z podzia³em na segmenty.

Wszyscy u¿ytkownicy generowani przez LoadRunnera kontrolowani s± z jednego centralnego modu³u zwanego kontrolerem. Testuj±c 
system pod obci±¿eniem mo¿na symulowaæ ró¿ne numery IP klientów, ró¿ne przegl±darki internetowe oraz ró¿ne 
szybko¶ci ³±czy internetowych, ca³y czas jednocze¶nie monitoruj±c zachowanie naszego obci±¿anego systemu.
Dane Techniczne Narzêdzia:
\begin{description}
\item[Wspierane protoko³y typu klient/serwer]\
	\begin{itemize}
	\item Oracle UPI
	\item Oracle OCI
	\item ODBC
	\item MS-SQL Server
	\item Sybase ctlib
	\item Sybase dblib
	\item Informix I-Net
	\item DB2 CLI
	\item Tuxedo (including compression mode)
	\item RTE
	\item CORBA
	\item COM/DCOM
	\item WinSocket
	\end{itemize}
\item[Wspierane protoko³y ERP]\
	\begin{itemize}
	\item SAP R/3
	\item PeopleSoft (2-tier i Tuxedo-based)
	\item Oracle Applications
	\item Siebel
	\item Baan
	\end{itemize}
\item[Wspierane protoko³y internetowe]\
	\begin{itemize}
	\item Streaming (Real Audio and Real Video)
	\item MS Media
	\item iMode
	\item VoiceXML
	\item LDAP
	\item WAP--HTTP
	\item WAP--Gateway
	\item HTTP
	\item HTTPS (SSL)
	\item Digital Certificates
	\item RMI
	\item FTP
	\item POP3
	\item Winsock
	\item SMTP
	\end{itemize}
\item[Wspierane systemy Legacy]\
	\begin{itemize}
	\item 3270 Terminals (Mainframe)
	\item 5250 Terminals (AS/400)
	\item VT    Terminal   (DEC)
	\item X Window Applications
	\end{itemize}
\item[Tworzenie testów wydajno¶ciowych:]\
	\begin{itemize}
	\item Mo¿liwo¶æ prze³±czania rodzaju nagrywanego protoko³u w trakcie rejestracji jednego skryptu;
	\item Narzêdzie automatycznie generuje skrypty testów obci±¿eniowych, oparte na operacjach biznesowych wykonywanych na 
	testowanej aplikacji;
	\item Narzêdzia do automatycznej parametryzacji skryptów pozwalaj± na szybk± i wydajn± obróbkê stworzonego testu;
	\item Przypisywanie ró¿nych numerów IP do poszczególnych symulowanych u¿ytkowników daje du¿e mo¿liwo¶ci w tworzeniu 
	scenariuszy testowych;
	\item Automatyczna kontrola zawarto¶ci danych w czasie testów wydajno¶ciowych, stanowi±ca jednocze¶nie kontrolê funkcjonaln±;
	\item Automatyczne korelowanie dynamicznie zmieniaj±cych siê danych przy nagrywaniu i odtwarzaniu skryptów;
	\item Korelacja zapytañ Oraclowych, polegaj±ca na mo¿liwo¶ci przechwytywania danych pobieranych z bazy i wykorzystywaniu ich w 
	dalszej czê¶ci skryptu;
	\item Emulacja po³±czenia modemowego o ró¿nej przepustowo¶ci;
	\item Kreator scenariuszy testowych, daj±cy mo¿liwo¶æ szybkiego i efektywnego tworzenia ca³ych scenariuszy testowych.
	\end{itemize}

\item[Kontrolowanie testów wydajno¶ciowych]\
	\begin{itemize}
	\item Automatyczna kontrola i synchronizacja wszystkich symulowanych u¿ytkowników z jednego centralnego punku kontroli;
	\item Monitorowanie w czasie rzeczywistym wszystkich parametrów systemu;
	\item £atwy w u¿yciu graficzny interfejs, umo¿liwiaj±cy tworzenie, uruchamianie, monitorowanie i analizowanie wyników testu;
	\item Jeden wspólny punk kontroli dla ca³ego testu z mo¿liwo¶ci± uruchamiania u¿ytkowników na systemach Windows i Unix.
	\end{itemize}

\item[Pomiary i kontrola wydajno¶ci]\
\begin{itemize}
	\item Mo¿liwo¶æ pomiaru i kontroli czasu wykonywania siê mierzonych transformacji w zadanych kryteriach lub od ,,zapytania -- 
	do odpowiedzi'' tzn. ³±cznie z czasem przetwarzania klienta, serwera i sieci komputerowej;
	\item Mo¿liwo¶æ wygenerowania dedykowanego obci±¿enia serwera, powielaj±c zarejestrowan± transmisjê po konkretnym protokole;
	\item Pomiar parametrów systemu operacyjnego dzia³aj±cego na obci±¿anym serwerze, daj±cy mo¿liwo¶æ znalezienia s³abych punktów 
	konfiguracji i zasobach systemu operacyjnego;
	\item Pomiar parametrów sprzêtowych maszyny bêd±cej obci±¿anym serwerem, daj±cy mo¿liwo¶æ znalezienia s³abych i niewydolnych 
	punków w konfiguracji sprzêtowej testowanej maszyny;
	\item Mo¿liwo¶æ pomiaru czasów wykonywania siê transakcji zdefiniowanych przez testera.
	\end{itemize}
\item[Analiza i kontrola wyników, dostêpne monitory:]\
	\begin{itemize}
	\item Server Resource Monitor ( NT, Unix, Linux) -- pozwala na kontrolowanie zasobów sprzêtowych maszyn bêd±cych 
	testowanymi serwerami;
	\item Network Delay Monitor -- pozwala na kontrolowanie czasów odpowiedzi i realizowania siê monitorowanych transakcji we 
	wszystkich segmentach sieci obci±¿anego systemu;
	\item SNMP Monitor -- pozwala na kontrolowanie przep³ywu danych pomiêdzy poszczególnymi elementami aktywnymi sieci jak: 
	Routery, Huby, Bridge;
	\item WebSphere, ColdFusion, SilverStream, BroadVision, ATG Dynamo, GemStone/J, Ariba Buyer, WebLogic, MS ASP, iPlanet -- 
	monitory serwerów aplikacji internetowych, posiadaj±ce specjalne wsparcie dla ka¿dego z nich;
	\item EJB, TUXEDO -- kontroluje i monitoruje wszystkie parametry systemu opartego na aplikacjach TUXEDO i EJB;
	\item MS IIS, Netscape, Apache -- monitory serwerów webowych;
	\item RealServer, Windows Media Server -- monitory do kontrolowania technologii strumienowych;
	\item COM/CORBA -- monitory do kontrolowania i badania technologii rozproszonych;
	\item ORACLE, SQLServer -- monitory do ¶ledzenia i badania wydajno¶ci serwerów baz danych, zawieraj±ce specjalne wsparcie i 
	integracje z tym technologiami.
	\end{itemize}

\item[Wirtualni u¿ytkownicy s± odtwarzani na systemach:]\
	\begin{itemize}
	\item Windows NT
	\item Sun Solaris
	\item HP--UX
	\item IBM AIX
	\item Linux
	\end{itemize}

\end{description}



\section{Projekt systemu do zarz±dzania wielokomputerowymi serwerami WWW w oparciu o IBM SecureWay Network Dispatcher}

Ca³o¶æ zadania polega³a na stworzeniu jednej architektury systemu WWW którego witryna zawiera³aby oko³o 60\% zawarto¶ci
w postaci stron dynamicznych (wype³nianych danymi pochodz±cymi ze znajduj±cego siê w uk³adzie serwera baz danych) generowanych
on line. Jednak¿e rozdysponowanie obci±¿enia pomiêdzy poszczególnymi elementami serwisu WWW by³o realizowane na trzy sposoby: 
statycznie, dynamicznie oraz w zale¿no¶ci od typu nadchodz±cych do serwera ¿±dañ. Taka architektura mia³a daæ odpowied¼ na 
pytanie -- jak zaprojektowaæ sytem WWW obs³uguj±cy strony generowane dynamicznie w mo¿liwie najbardziej efektywny sposób.

\subsection{Architektura}

W eksperymencie wykorzystano lokaln± sieæ komputerow± z³o¿on± z jednego segmentu sieci FastEthernet. W jego sk³ad wchodzi³y
komputery, na których zainstalowane by³o oprogramowanie generuj±ce obci±¿enie dla klastera serwerów WWW (razem dziewiêæ sztuk)
pracuj±cych pod kontrol± systemu Windows NT 4.0 Server (Service Pack 6a).
 
\begin{figure}[h]
\centering
\includegraphics[width=14cm]{./rysunki/komputerki.eps}
\caption{Architektura segmentu sieci testowej.}
\label{architektura}
\end{figure}
Ka¿dy komputer wyposa¿ony by³ w procesor Intel 
Celeron 300 MHz, 96 MB pamiêci SDRAM, dysk twardy o pojemno¶ci 3 GB oraz kartê sieciow± 3COM EtherLink XL (10/100 Mbps). 
P³yta g³ówna ka¿dego komputera zbudowana by³a w oparciu o chipset Intel 430 LX. Komputery te posiada³y adresy IP z zakresu 
156.17.130.69 -- 156.17.130.79 i nazwy odpowiednio Beryl04 -- Beryl12. Na komputerach tych zainstalowane by³o oprogramowanie
Astra LoadRunner firmy Mercury Interactive. Komputery o identycznej architekturze stanowi³y serwery baz danych: beryl01 (IP = .66), beryl02 (IP = .67), beryl03 (IP = .68).
Zainstalowano na nich MS SQL server w wersji 7.0 (wersja 120 dniowa) wraz z baz± testow±.

Poza wy¿ej wymienionymi komputerami PC pod³±czony by³y badany klaster serwerów WWW. Sprzêtow± platform± serwerów by³y 
komputery IBM RS 6000 43P Model 260 pracuj±ce pod kontrol± systemu operacyjnego AIX 4.3.3. Ka¿dy z tych komputerów posiada³ 
256 MB pamiêci RAM, dysk twardy UW SCSI o pojemno¶ci 4.3 GB oraz zintegrowan± kartê sieciow± IBM 10/100 Mbps. ,,Sercem'' tej 
serii komputerów RS 6000 jest procesor RISC PowerPC Power3 taktowany czêstotliwo¶ci± 200 Mhz. Trzy z wykorzystywanych maszyn 
posiada³y jeden taki procesor a dwa wyposa¿one by³y w dwa procesory po³±czone w architekturze SMP. Na ka¿dym z badanych 
serwerów zainstalowane by³o oprogramowanie serwera WWW Apache for AIX. Poszczególne komputery RS 6000 posiada³y adresy IP z 
zakresu 156.17.130.45 -- 156.17.130.49 i nazwy odpowiednio akwamaryn (RS/6000 dwuprocesorowy), szafir (dwuprocesorowy), 
opal01 -- opal03 (jednoprocesorowe). Jedna maszyna dwuprocesorowa (szafir) pos³u¿y³a do 
rozpropagowania obci±¿enia (dispatcher+CBR i WTE), druga pos³u¿y³a do obs³ugi ¿±dañ o statyczne
pliki HTML, za¶ pozosta³e maszyny obs³ugiwa³y dynamicznie generowane (poprzez skrypty PHP) strony,
w ten sposób, ¿e ka¿dy z opali pobiera³ dane z bazy danych poszczególnych beryli (opal01--beryl01,
 opal02--beryl02 i opal03--beryl03).

Wszystkie komputery by³y po³±czone ze 100Mbps switchem OmniS/R-5 firmy Alcatel. Równie¿ wyj¶cie na Swiat by³o realizowane z
t± szybko¶ci±. 

Konstrukcja zarz±dzanego, wielokomputerowego systemu WWW by³a bardzo z³o¿ona. Sk³ada³a siê z 
nastêpuj±cych elementów:
\begin{enumerate}
\item Instalacja i konfiguracja switcha OmniSwitch/Router firmy Alcatel;
\item Instalacja i konfiguracja dispatchera z modu³em CBR. W sk³ad tej czê¶ci wesz³y: 
\begin{itemize}
\item instalacja systemu AIX w wersji 4.3.3 oraz jego update do wersji 4.3.3.0.8 (niezbêdne pliki
zosta³y ¶ci±gniête z odpowiedniej strony firmy IBM;
\item instalacja wirtualnej maszyny (VM) Javy w wersji 1.3.0;
\item instalacja oprogramowania IBM WebSphere Edge Server 1.0.3  wraz z 60--dniow± licencj±;
\item skonfigurowanie WTE jako \emph{reverse proxy } o adresie URL: www1.ists.pwr.wroc.pl, który
przekierowywa³ adresy URL do serwerów: opal01, opal02, opal03 i akwamaryn, w ten sposób, aby
¿±dania o statyczne pliki HTML zosta³y skierowane do serwera akwamaryn, za¶ ¿±dania o dynamicznie
generowane strony z rozszerzeniami: .PHP, .PHP3 i PHP4 zosta³y rozpropagowane na pozosta³e
trzy maszyny;
\end{itemize}
\item Instalacja i konfiguracja oprogramowania dodatkowego na poszczególne serwery WWW i 
dispatchera tzn. HTTP Server Apache w wersji 1.3.19, kompilator C i C++ w postaci gcc w wersji
2.95.3 oraz jêzyka skryptowego PHP w wersji 4.0.6;
\item Instalacja i konfiguracja Windows NT 4.0 Server wraz z Service Pack 6.0 na dwunastu
laboratoryjnych komputerach PC;
\item Instalacja i konfiguracja Microsoft SQL Serwera w wersji 7.0 na serwerach baz danych oraz
instalacja bazy testowej;
\item Instalacja oprogramowania do testów na pozosta³ych komputerach; przygotowanie testu;
\item przygotowanie witryny WWW do testów;
\end{enumerate}

\subsection{Algorytmy i metody}
Poni¿ej znajduj± siê opisane trzy powsta³e modele. W ka¿dym z nich program komputer (RS/6000) wraz z zainstalowanym programem 
IBM SecureWay Network Dispatcher stanowi uk³ad rozdysponuj±cy ¿±daniami. 

\subsubsection{Information less -- modu³ dispatcher + Round Robin}

Sam Dispatcher nie ma mo¿liwo¶ci pracy w statycznym algorytmie Round Robin, jednak¿e (jak napisano powy¿ej, w charakterystyce 
oprogramowania) mo¿na pos³ugiwaæ siê modu³em Dispatcher z wy³±czonym modu³em Zarz±dcy oraz z ogórnie nadanymi wagami. W tym
przypadku, pomimo niehomogenicznego ¶rodowiska nadano wszystkim elementom klastra WWW wagi równe jeden. W tej sytuacji komputer
z zainstalowanym dispatcherem równomiernie (bez jakiejkolwiek analizy) propagowa³ ¿±dania do poszczególnych komputerów.

W tym przypadku dostêp do bazy danych móg³ byæ realizowany z dowolnego komputera (serwera WWW).

\subsubsection{Server info aware -- modu³ dispatcher + Weighted Round Robin}

W porównaniu do poprzedniego przypadku w tym u¿yto Dispatchera z uruchomionym modu³em zarz±dcy. Wtedy te¿ realizowany by³
algorytm Wa¿ony Round--Robin. Wagi nadawane by³y dynamicznie tzn. w zale¿no¶ci od obci±¿enia poszczególnych komputerów -- 
serwerów WWW. 

W tym jak i w powy¿szym przypadku dostêp do bazy móg³ byæ równie¿ realizowany z dolwolnego z serwerów WWW.

\subsubsection{Client info aware -- modu³ CBR + WTE}

By³ to najbardziej z³o¿ony przypadek. Ka¿dy pakiet by³ analizowany pod wzglêdem typu zawarto¶ci oraz pod wzglêdem jego 
obecno¶ci w keszu komponentu WTE. CBR zosta³ tak skonfigurowany aby rozpoznawaæ URL z zawartym w nim wyrazem .php 
(identyfikuj±cym plik HTML z zawartym kodem PHP dostêpu do bazy). Pozwoli³o to na rozdzdzia³ zapytañ na ¿±dania o pliki
statyczne (te by³y rozsy³ane go trzech jednoprocesorowych maszyn RS/6000), za¶ te ¿±dania o pliki, których czê¶æ by³a
generowana dynamicznie by³y przekazywane na dwuprocesorowy serwer, który komunikowa³ sie (tylko on) z serwerem baz danych.
Taka konfiguracja pozwoli³a rozdysponowaæ zarówno obci±¿enie jak i typ po³±czeñ.

\subsection{Metodologia testowania}

Celem testowania powsta³ego systemu do zarz±dzania wielokomputerowym serwerem WWW
jest zbadanie wydolno¶ci takiego systemu -- tzn. jego wydajno¶ci w sytuacji
maksymalnego obci±¿enia, czyli tak¿e mo¿liwo¶ci odpowiedzi na klienckie 
¿±danie w takiej sytuacji. Zamierzeniem testowania jest odpowied¼ na 
pytanie jak obci±¿enie jest równowa¿one pomiêdzy poszczególnymi czê¶ciami
sk³adowymi systemu, w zale¿no¶ci od algorytmu dysypacji obci±¿enia
(Round--Robin, Weighted Round--Robin oraz rozproszenie oparte na CBR), oraz
jaki dany algorytm ma wp³yw na LB.

Testowanie zaprojektowanego rozproszonego serwera WWW polega³o na pomiarze
wydajno¶ci tak wej¶ciowej jak i wyj¶ciowej ca³ego systemu jak i poszczegolnych
jego sk³adowych (poszczególnych serwerów WWW, serwera baz danych i dispatchera).

Celem testowania by³o zbadanie maksymalnej wydajno¶ci systemu. Wydajno¶æ systemu mo¿na badaæ na kilka sposobów:
\begin{itemize}
\item ilo¶æ zapytañ w czasie,
\item ilo¶æ danych przes³anych w czasie.
\end{itemize}

Na wynik wp³yw ma informacja w jaki sposób roz³o¿one by³y zapytania oraz czy sesje koñczy³y siê prawid³owo. Za³o¿ono trzy 
mo¿liwo¶ci odpowiedzi systemu:
\begin{itemize}
\item w czasie do 3 s.,
\item w czasie 5 s.,
\item odpowiedz powy¿ej 8 s.
\end{itemize}

Za sesjê prawid³owo zakoñczon± uznno tak±, podczas której dwie kolejne odpowiedzi na pytania nie przekroczy³y 8 s.

Za³o¿ono nastêpuj±ce ¶cie¿ki poruszania siê po serwisie \cite{savoia3}:
\begin{itemize}
\item \emph{Home ---> Exit} (Home -- strona domowa);
\item \emph{Home ---> First Level ---> Exit} (First Level -- np. informacja o produkcie);
\item \emph{Home ---> First Level ---> Object ---> Exit} (Object -- np. zakup).
\end{itemize}
Przy za³o¿eniu, ¿e ruch na ,,typowej'' witrynie \cite{savoia1,savoia2,savoia3} wygl±da w nastêpuj±cy sposób:
\begin{itemize}
\item \emph{Home} -- 58 \%;
\item \emph{First Level} -- 31 \%;
\item \emph{Object} -- 11 \%;
\end{itemize}

Na bazie powy¿szych danych nale¿y zbudowaæ skrypty generuj±ce zapytania.

\subsubsection{Analiza wyników}

Aby prawid³owo odpowiedzieæ na pytanie zwi±zane z wydajno¶ci± i dostêpno¶ci± systemu nale¿y podaæ, 
przy jakiej ilo¶ci u¿ytkowników system jest w stanie prawid³owo odpowiadaæ czyli
przy \emph{x} u¿ytkowników (otwartych sesji) system odpowiada (pomiary zostaj± wykonanywane w okre¶lonych przedzia³ach 
czasowych):
\begin{itemize}
\item \% w czasie do 3 s.
\item \% w czasie do 5 s.
\item \% powy¿ej 8 s.
\end{itemize}

Je¿eli podczas sesji dwie nastêpuj±ce po sobie odpowiedzi przekroczy³y 8s. -- oznacza to, ¿e ich sesje zosta³y zerwane.
System nie mo¿e byæ obci±¿ony w wiêkszym stopniu ni¿ aktualny aby wszystkie nap³ywaj±ce do niego sesje zosta³y 
prawid³owo obs³u¿one.

Tak zaplanowane testy nale¿y wykonaæ podczas pracy Dispatcher--a z ró¿nymi algorytmami: Round Robin, Multi Class Round Robin,
Weighted Round Robin. Wyniki powy¿szych testów pozwalaj± sprawdziæ, jak zachowuje siê system w przypadku chwilowego du¿ego 
obci±¿enia. 

Kolejnym elementem testów by³oby zasymulowanie normalnych warunków pracy systemu na granicy 80 \% wydajno¶ci. Podczas takiej 
pracy nale¿a³oby wygenerowaæ obci±¿enie znacznie przewy¿szaj±ce mo¿liwo¶ci systemu. Obci±¿enie to powinno trwaæ 3s, 5s, wiêcej 
ni¿ 8s. Pozwoli to sprawdziæ jak system reaguje i jak szybko jest w stanie siê ustabilizowaæ doprowadzaj±c do równego 
roz³o¿enia obci±¿enia na poszczególne serwery. 
\section{Wyniki wstêpnych eksperymentów}

Na skutek powa¿nych problemów, tak sprzêtowych jak i programowych autor pracy nie zdo³a³ 
zrealizowaæ wszystich zaplanowanych zadañ. Poni¿ej wymieniono kolejne elementy budowy systemu do zarz±dzania wielokomputerowym
serwerem WWW oraz stopieñ ich realizacji. 
\begin{enumerate}
\item przygotowano ¶rodowisko serwerów front--endowych oraz dispatchera:
\begin{itemize}
\item uruchomiono oraz zainstalowano system operacyjny AIX 4.3.3 na komputerach IBM RS/6000, ktore mia³y stanowiæ serwer
dispatchera, oraz cztery serwery WWW; na tych równie¿ maszynach wykonano niezbêdny update do wersji 4.3.3.0.8;
\item zainstalowano menad¿er pakietów RPM, kompilator gcc 2.95.3, Apache 1.3.19 oraz jêzyk skryptowy PHP 4.0.6 (za jego pomoc±
nale¿a³o stworzyæ interface pomiêdzy serwerem WWW a serwerem baz danych) -- jako modu³ ³adowalny do serwera Apache;
\item skonfigurowano ka¿dy z serwerów WWW wraz z modu³em ³adowalnym PHP;
\item zainstalowano i skonfigurowano IBM WebSphere Edge Server jako \emph{reverse proxy} wraz z modu³em CBR odpowiedzialnym
za specyficzne, oparte na regu³ach przekierowywanie ¿±dañ; podczas instalacji napotkano jednak¿e na dwa powa¿ne problemy, które
uniemo¿liwia³y instalacjê, a tym samym dzia³anie pakietu: modu³ dispatcher nie uruchamia³ siê zg³aszaj±c brak pliku licencji,
za¶ WTE nie uruchamia³ siê z powodu niekompatybilno¶ci z g³ówn± bibliotek± systemow± (libc.a) (na obie sytuacje nie znaleziono
pomocy w ¿adnej posiadanej literaturze); skorzystano z \emph{supportu} firmy IBM; w pierwszym przypadku -- otrzymano poprawny
plik licencyjny, za¶ w drugim rozwi±zaniem okaza³o siê prze³±czenie urz±dzeñ I/O w tryb asynchroniczny; po wykonaniu obu zadañ
oraz skonfigurowaniu modu³ów -- dispatcher + WTE dzia³a³o poprawnie;
\end{itemize}
\item przygotowano ¶rodowisko serwerów back-endowych (baz danych):
\begin{itemize}
\item zainstalowano na trzech komputerach PC system operacyjny Windows NT 4.0 Server wraz z serwerem baz danych Microsoft
SQL Server 7.0 (wersja 120--dniowa), nastêpnie skonfigurowano te maszyny;
\item zainstalowano bazê testow± na serwerach;
\end{itemize}
\item system testowy Astra LoadRunner; niestety nie uda³o siê uzyskaæ tego oprogramowania testowego z powodu braku wersji 
testowej (komercyjna kosztuje oko³o 13000\$); uzyskano jednak¿e program Astra LoadTest o zbli¿onej funkcjonalno¶ci (wersja 
testowa na dzisiêciu wirtualnych u¿ytkowników dzia³aj±ca bez ograniczeñ 7 dni); zosta³ zainstalowany i sprawdzony w kierunku
wymagañ tej pracy; wykonano za jego pomoc± wstêpny, przyk³adowy test, opis i wyniki znajduj± siê 
w Dodatku;
\end{enumerate}

Po poprawnym wykonaniu wy¿ej wymienionych czynno¶ci autor pracy stan±³ przed problemem pobierania danych z serwerów baz danych
przez serwery WWW. Pakiet RPM jêzyka PHP zawiera³ binarny modu³ ³adowalny do Apache skompilowany bez wsparcia w komunikacji
z bazami danych (poza samym modu³em ³adowalnym istniej± jeszcze modu³y odpowiadaj±ce komunikacji z poszczególnymi serwerami
baz danych). W zwi±zku z tym, ¿e PHP jest jêzykiem, który jest dostêpny w ¼ród³ach (na licencji GPL) próbowano poprzez 
kompilacjê i odpowiedni± konfiguracjê w³±czyæ go do Apache. MS SQL Server jest serwerem baz danych opartym na motorze firmy 
Sybase, dlatego aby nawi±zaæ komunikacjê z serwera WWW do serwera bazy danych MS SQL poprzez PHP nale¿a³o przygotowaæ 
(przekompilowaæ) odpowiednie modu³y (klienty) PHP w formie modu³ów ³adowalnych. Niestety z powodu braku odpowiedniego API
nie uda³o siê tego zadania zrealizowaæ (aby móc stworzyæ klienty PHP do baz danych -- nale¿y kompilowaæ PHP wraz z API
odpowiedniego serwera bazy danych).

Próbowano tak¿e skorzystaæ z odpowiednich sterowników ODBC firmy OpenLink\footnote{http://www.openlinksw.com}. Poza 
ograniczeniami w korzystaniu z tego oprogramowania (mo¿liwo¶æ pracy tylko dwóch jednoczesnych u¿ytkowników podczas sesji) nie
uda³o siê przekompilowaæ PHP (wystêpowa³y b³êdy podczas kompilacji). Podczas poszukiwania rozwi±zania w Internecie, skorzystano
równie¿ z mo¿liwo¶ci Usenetu (listy dyskusyjne).

Powy¿szego problemu nie uda³o siê równie¿ rozwi±zaæ analizuj±c zmianê jêzyka oprogramowania: PERL (kolejny znakomity jêzyk 
skryptowy umo¿liwiaj±cy generowanie stron dynamicznych) -- ma zbli¿one wymagania je¶li chodzi o klientów baz danych, ASP (nie 
istnieje na maszyny Unixowe), C, C++ lub Java (du¿e k³opoty implementacyjne, spora pracoch³onno¶æ).

W zwi±zku z brakiem mo¿liwo¶ci scalenia sewerów WWW z serwerami baz danych nie uda³o siê przetestowaæ w pe³ni dzia³aj±cego
systemu do zarz±dzania wielokomputerowym serwisem WWW.
