\chapter{Zarz±dzanie wielokomputerowym serwisem WWW}
\label{r04}
W rozdziale tym znajdzie siê próba odpowiedzi na pytanie co to jest kiedy i po co stosuje siê zarz±dzanie serwerem 
WWW (jakie s± parametry charakterystyczne oceny wydajno¶ci i dostêpno¶ci). Nastêpnie zostanie przedstawiona szczegó³owa 
taksonomia serwerów WWW. Ostatnim punktem tego rozdzia³u 
bêdzie porównanie us³ug zapewnianych przez witryny statyczne i dynamiczne, opis zastosowañ i mo¿liwo¶ci obu 
typów tworzenia witryn oraz wymagania stawiane systemom WWW wraz z pogl±dowym przyk³adem.

\section{Wprowadzenie}

Jak zauwa¿ono wcze¶niej -- przysz³o¶æ serwisów WWW nale¿y do platform wieloserwerowych. Wynika to z mo¿liwo¶ci 
teoretycznie niograniczonej rozbudowy wraz ze wzrostem liczby u¿ytkowników oraz ich wymagañ. Kolejn± zalet± jest 
równie¿ spe³nianie bezpieczeñstwa przez takie wielokomputerowe systemy -- w dowolnym elemencie takiego systemu -- nodzie --
mo¿e nast±piæ praktycznie dowolnego rodzaju uszkodzenie -- tak dysk, jak pamiêæ operacyjna czy procesor -- wtedy poszczególne
procesy s± transportowane na inne nody, nawet w przypadku uszkodzenia jednej maszyny, nie spowoduje to przestoju. Jednak¿e aby
taki system spe³nia³ dobrze swoje zadania potrzebne jest oprogramowanie lub urz±dzenie pozwaj±ce nim zarz±dzaæ czyli 
odpowiednio rozdysponowaæ ¿±dania klienckie oraz wykrywaæ uszkodzenia poszczególnych elementów uk³adu. W literaturze
przyjê³o siê takie rozdysponowanie zapytañ pomiêdzy poszczególne nody nazywaæ równowa¿eniem obci±¿enia systemu\footnote{ang. 
\emph{load balancing, LB}}. 

\section{Równowa¿enie obci±¿enia -- metody}

Równowa¿enie obci±¿eñ (ang. \emph{load balancing}) w systemach rozproszonych, to zagadnienie z dziedziny 
rozdzia³u zadañ polegaj±ce na dystrybucji pomiêdzy wêz³y systemu nap³ywaj±cych do niego zleceñ tak, aby  
maksymalizowaæ jego ³±czn± wydajno¶æ. Oznacza to, ¿e dzia³alno¶æ zwi±zana z rozdzia³em zadañ nie mo¿e powodowaæ 
obci±¿enia systemu w stopniu, który niwelowa³by korzy¶ci wynikaj±ce z faktu zrównowa¿enia obci±¿eñ jego wêz³ów. 
Angielski termin load balancing w rzeczywisto¶ci funkcjonuje jako reprezentant tej problematyki, gdy¿ traktowany 
¶ci¶le oznacza doprowadzenie systemu rozproszonego do stanu w którym wszystkie jego wêz³y obci±¿one s± w 
dok³adnie równym stopniu, nawet je¿eli oznacza³oby to odebranie zadañ jednemu z nich. W stosunku do wiêkszo¶ci 
rozwi±zañ przemys³owych z tego zakresu nale¿a³oby poprawnie u¿ywaæ okre¶lenia wspó³dzielenie obci±¿eñ (ang. \emph{
load sharing}) lub bardziej ogólnie, rozdzia³ obci±¿eñ (ang load distribution). Realizacja ¶cis³ego równowa¿enia 
obci±¿eñ wi±¿e siê z konieczno¶ci± implementacji metod odbierania zadañ wêz³om, powoduje to wzrost z³o¿ono¶ci 
realizacji algorytmu, a sam proces odbierania zadania jest do¶æ kosztowny. Wzrost wydajno¶ci uzyskiwany dziêki 
idealnemu zrównowa¿eniu obci±¿eñ w systemie w porównaniu do wydajno¶ci uzyskiwanej przy zastosowaniu 
wspó³dzielenia obci±¿eñ nie jest zwykle na tyle du¿y, by usprawiedliwia³ ponoszenie kosztów zwi±zanych z 
pokonaniem z³o¿ono¶ci realizacji ¶cis³ego równowa¿enia obci±¿eñ \cite{barylo13,barylo16,barylo17}. W tej pracy, tak jak w 
literaturze angielskojêzycznej, u¿ywany jest termin równowa¿enie obci±¿eñ, chyba ¿e zaznaczono inaczej.
Wydaje siê, ¿e najskuteczniejsz± metod± zwiêkszenia wydajno¶ci serwera WWW jest powielenie (lub podzia³) danych, które 
serwer 
udostêpnia, pomiêdzy grupê po³±czonych sieci± komputerów (dodatkowych serwerów WWW) i zapewnienie dostêpu do 
tej grupy tak jak do pojedynczego serwera z uwzglêdnieniem transparentnego dla u¿ytkownika i maksymalizuj±cego 
wydajno¶æ podzia³u ¿±dañ HTTP pomiêdzy poszczególne elementy grupy. Taka grupa nazywana klastrem lub farm± 
serwerów stanowi swoisty system rozproszony, którego wydajno¶æ mo¿na maksymalizowaæ stosuj±c równowa¿enie 
obci±¿eñ. Nale¿y zaznaczyæ, ¿e chocia¿ termin klaster oznacza serwery po³±czone w ca³o¶æ logiczn± (dostêpne pod 
jedn± nazw± lub adresem IP), nie implikuj±c ¿adnych ograniczeñ na geograficzne rozproszenie serwerów, to wiele 
rozwi±zañ komercyjnych wymaga, by serwery nale¿±ce do klastra znajdowa³y siê w obszarze jednej sieci lokalnej 
(mia³y jednakow± czê¶æ globaln± adresu IP).

\section{Przegl±d i podzia³ webowych algorytmów szeregowania}

Wszystkie mechanizmy realizuj±ce zarz±dzanie dostêpem do rozproszonych serwerów WWW musz± mieæ zaimplementowany konkretny 
algorytm, na podstawie którego bêd± mog³y podejmowaæ decyzjê o przes³aniu zapytania do najlepszego serwera. W zale¿no¶ci od 
stopnia skomplikowania systemu i mo¿liwo¶ci implementacyjnych wykorzystywane algorytmy mog± mieæ ró¿n± z³o¿ono¶æ obliczeniow±. 
Zastosowane strategie zale¿ne s± równie¿ od informacji, jakie posiada system na temat ilo¶ci zapytañ generowanych przez 
klientów oraz informacji o stanie serwerów wchodz±cych w sk³ad rozproszonego systemu WWW. Mo¿na je podzieliæ na kilka 
kategorii w zale¿no¶ci od ilo¶ci informacji, jaka wykorzystywana jest przy podejmowaniu decyzji. 

\subsection{Algorytmy statyczne (nie wykorzystuj±ce informacji zewnêtrznych)}
Je¿eli w procesie szeregowania nie s± wykorzystywane ¿adne informacje o stanie serwerów lub strumieniu zapytañ, mówi siê o 
algorytmach statycznych. Te mechanizmy, to najprostsze rozwi±zania gwarantuj±ce w niewielkim stopniu roz³o¿enie obci±¿enia 
pomiêdzy serwery. Rozk³ad ten jest bardzo przypadkowy i nie gwarantuje równomiernego wykorzystania zasobów ani wysokiej 
dostêpno¶ci systemu, jednak¿e s± one najszybszymi, gdy¿ wymagaj± najmniej mocy obliczeniowej. Do takich algorytmów nale¿±:
\begin{itemize}
\item Random -- system nie posiada ¿adnych informacji o stanie serwerów WWW, jak równie¿ nie posiada informacji o tym, do 
którego serwera ostatnio skierowane zosta³o zapytanie;
\item Round--robin -- system nie posiada ¿adnych informacji o stanie serwerów WWW, posiada jednak informacjê historyczn± o tym, 
do którego serwera skierowane zosta³o ostatnio nades³ane zapytanie.
\end{itemize}

\subsection{Algorytmy dynamiczne (wykorzystuj±ce informacjê zewnêtrzn±)}
W procesie szeregowania mo¿na stosowaæ bardziej rozbudowane mechanizmy zarz±dzania. Aby zwiêkszyæ efektywno¶æ szeregowania, 
mo¿na stosowaæ strategie wykorzystuj±ce informacje o kliencie. Wybór najlepszego serwera mo¿e odbywaæ siê na podstawie adresu 
IP lub numer portu TCP wykorzystywanego przez klienta. Mo¿liwe jest równie¿ wykorzystywanie tzw. alarmów generowanych przez 
serwery. Do mechanizmu decyduj±cego o wyborze serwera w okre¶lonych odstêpach czasowych przekazywana jest informacja zwrotna o 
obci±¿eniu serwera lub informacja o przekroczeniu jakiego¶ okre¶lonego parametru. Szeregowanie mo¿e odbywaæ siê na podstawie 
jednej z metryk okre¶laj±cych obci±¿enie:
\begin{itemize}
\item ilo¶ci aktywnych po³±czeñ z serwerem,
\item wykorzystania dysku i/lub procesora serwera,
\item przewidywanego strumienia zapytañ w okre¶lonym czasie.
\end{itemize}

W przypadku modeli, gdzie mechanizmy szeregowania w ca³o¶ci kontroluj± strumieñ zapytañ, mog± byæ stosowane algorytmy 
dystrybuuj±ce zapytania w zale¿no¶ci od wielko¶ci nap³ywaj±cego strumienia oraz obci±¿enia serwerów obs³uguj±cych te 
zapytania. Dziêki informacjom zbieranym od serwerów mo¿liwe jest szacowanie parametrów maj±cych wp³yw na szybko¶æ ich 
odpowiedzi. Mog± to byæ:
\begin{itemize}
\item ilo¶æ zapytañ zrealizowana w okre¶lonym czasie,
\item obci±¿enie procesora w danym momencie,
\item poziom wykorzystania dysków w serwerze.
\end{itemize}
Takie rozwi±zania umo¿liwiaj± wyeliminowanie przeci±¿onych serwerów. W znacznym stopniu zwiêksza to dostêpno¶æ klientów do 
danych oraz wp³ywa na lepsze wykorzystanie zasobów. 

Algorytmy dynamiczne mo¿emy w zale¿no¶ci od miejsca analizy podzieliæ na:
\begin{itemize}
\item \emph{Server Info Aware} -- czyli wykorzystuj±ce informacje o stanie serwera;
\item \emph{Client Info Aware} -- czyli wykorzystuj±ce specyfikê ¿±dañ klienckich -- przekierowuj±ce ¿±dania w zale¿no¶ci
od typu zapytania klienckiego (dane z serwera baz danych, wykorzystuj±ce skrypty CGI, zawieraj±ce pliki multimedialne itp.)
\end{itemize}

\subsection{Algorytmy szeregowania wykorzystywane po stronie serwera DNS}

\subsubsection{Algorytmy nie wykorzystuj±ce informacji o stanie systemu -- statyczne}

\begin{itemize}
\item Round--robin DNS\\
Podej¶cie to zosta³o zastosowane jako pierwsze przy budowie skalowalnego systemu serwerów webowych przez NCSA \cite{modele22}. Kod 
serwera DNS (np. Unixowy BIND) bez ¿adnych modyfikacji mo¿e wykorzystywaæ algorytm Round--robin. Obci±¿enie serwerów nie jest 
zbyt dobrze 
równowa¿one ze wzglêdu na mechanizm cache dla powi±zañ nazwa domenowa - adres IP. Stosuj±c takie podej¶cie, nie ma siê równie¿ 
kontroli nad dostêpno¶ci± serwerów oraz nie uwzglêdnia siê mo¿liwo¶ci zastosowania serwerów o ró¿nej wydajno¶ci.
Jest to algorytm bardzo prosty zarówno w dzia³aniu jak i implementacji (w stadardowym serwerze DNS mo¿na go zastosowaæ, bez
konieczno¶ci modyfikacji kodu serwera). Jego dzia³anie polega na cyklicznym rozsy³aniu pakietów do kolejnego na li¶cie serwera.
Oczywi¶cie najlepsze wyniki osi±ga siê dla komputerów symetrycznych sprzêtowo. Modyfikacj± tego algorytmu jest 
Weighted Round--Robin, w którym to algorytmie mo¿na statycznie nadaæ wagi poszczególnym serwerom w zale¿no¶ci od ich 
wydolno¶ci;
\item Random\\
\end{itemize}

\subsubsection{Algorytmy wykorzystuj±ce informacje o stanie systemu}

Alternatyw± dla algorytmu Round--robin najczê¶ciej stosowanego w przypadku serwera DNS s± algorytmy wykorzystuj±ce informacje o 
stanie systemu (wielko¶ci przewidywanego strumienia zapytañ, aktualnego obci±¿enia serwerów). Okazuje siê, ¿e regu³y bazuj±ce 
wy³±cznie na stanie obci±¿enia serwerów WWW s± nieefektywne. W rzeczywisto¶ci, w zwi±zku z hierarchicznym buforowaniem danych 
przez serwery DNS, pojedyncze zapytanie od klienta mo¿e spowodowaæ nap³yw wielu innych zapytañ. Wynika z tego, ¿e informacja o 
aktualnym obci±¿eniu nie jest w ¿aden sposób zwi±zana z przysz³ym obci±¿eniem \cite{ModeleFunkcjonalne}.
Podej¶cie pozwalaj±ce oszacowaæ nadchodz±cy strumieñ zapytañ (\emph{hidden load weight}) w czasie TTL (\emph{Time To Live}) jest najbardziej 
efektywne. Dziêki tym informacjom mo¿na przypisaæ ró¿ne warto¶ci TTL dla ró¿nych domen i estymowaæ wielko¶æ ukrytego, czyli 
niekontrolowanego przez serwer DNS strumienia zapytañ. Przyk³adami tego typu algorytmów s± \cite{modele14}:
\begin{itemize}
\item Multi tie round--robin -- dla ka¿dej domeny lub ich okre¶lonych zbiorów przypisywane s± ró¿ne warto¶ci TTL, pozwalaj±ce na
zrównowa¿enie strumieni zapytañ do ka¿dego serwera WWW.
\item Dynamically accumulated load (DAL) -- rozbudowana wersja algorytmu Multi tie Round--robin polegaj±ca na zbieraniu 
informacji o estymowanej wielko¶ci obci±¿enia serwerów i zmianie ³añcucha przypisañ adresów w serwerze DNS w zale¿no¶ci od 
zak³adanego obci±¿enia.
\item Minimum residual load -- modyfikacja algorytmu DAL polegaj±ca na estymowaniu wielko¶ci ukrytego obci±¿enia i warto¶ci TTL.
Po up³ywie TTL serwer WWW odpytywany jest o rzeczywist± wielko¶æ strumienia zapytañ, jaka zosta³a do niego skierowana. Dziêki 
temu algorytm ma informacjê o stanie serwera. Powoduje to równie¿ zmianê kolejno¶ci przypisañ w serwerze DNS;
\end{itemize}

\subsubsection{Algorytmy adaptacyjne}

Algorytmy równowa¿enia bazuj±ce na oszacowaniu hidden load weight oraz alarmach z krytycznie przeci±¿onych serwerów pozwalaj± 
na du¿o bardziej efektywne, w porównaniu z algorytmami Round--robin, równowa¿enie serwerów WWW. Niestety, s± one efektywne 
tylko w przypadku rozproszenia serwerów homogenicznych. Inn± grup± algorytmów przeznaczonych do równowa¿enia obci±¿enia 
serwerów heterogenicznych s± algorytmy adaptacyjne. W przypadku algorytmów adaptacyjnych warto¶æ TTL przypisywana jest 
dynamicznie do ka¿dego ¿±dania na podstawie przewidywanego hidden load weight oraz wydajno¶ci serwera, do którego zapytanie 
zosta³o przypisane. Idea tego podej¶cia polega na uwzglêdnieniu pewnej warto¶ci $\xi_i$ opisuj±cej moc obliczeniow± serwera $S_i$. 

Algorytmy te podzielone zosta³y na dwie grupy \cite{modele25}:
\begin{itemize}
\item probabilistyczne -- realizacja metody bazuje na algorytmie Round--robin. Za ka¿dym razem losowo generujemy liczbê 
$\gamma(0\leq\gamma\leq1)$. Je¿eli zapytania przypisane s± aktualnie do serwera $S_i$, to jako nastêpny zostanie 
wybrany $S_{i+1}$ tylko wtedy,
gdy $\gamma\leq\xi_i$, w przeciwnym przypadku warunek zostaje rozpatrzony dla kolejnego serwera. 
Warto¶æ TTL oblicza siê ze wzoru:
\begin{equation}
TTL_i = \frac{\eta}{\xi_i}
\end{equation}
\item deterministyczne -- ide± algorytmu jest takie dostosowanie czasu TTL, aby bardziej wydajne serwery obs³ugiwa³y wiêcej 
zapytañ, a mniej wydajne nie by³y przeci±¿ane. Dla ka¿dej domeny $i$ obs³ugiwanej przez system serwerów webowych zostaje 
przypisana warto¶æ TTL z  uwzglêdnieniem wydajno¶ci serwera $j$ wed³ug wzoru:
\begin{equation}
TTL_{ij} = \frac{\eta * c_i}{\xi_i}
\end{equation}
gdzie $\eta$ jest warto¶ci± sta³±, zale¿n± od liczby zdefiniowanych domen generuj±cych zapytania.
\end{itemize}

\subsection{Algorytmy szeregowania wykorzystywane w dystrybutorach}

Stosowane tutaj algorytmy  \cite{modele13} s± najprostsze, poniewa¿ dystrybutor obs³uguje wszystkie nadchodz±ce pakiety i konieczne jest 
zminimalizowanie czasu po¶wiêcanego na ich obs³ugê. Rekompensat± za konieczno¶æ obs³ugi wszystkich pakietów jest pe³na 
kontrola nap³ywaj±cego strumienia zapytañ. Stosowane w tym przypadku algorytmy mo¿na podzieliæ na trzy podstawowe grupy:
\begin{itemize}
\item Information less -- nie wykorzystuj±ce ¿adnych informacji o stanie systemu,
\item Client info aware -- wykorzystuj±ce informacje o adresie IP klienta lub numerze portu, przez który komunikuje siê z 
serwerem,
\item Server state aware -- wykorzystuj±ce informacje o stanie serwera, do którego przekazuj± zapytanie.
\end{itemize}

Tak jak w przypadku algorytmów stosowanych w serwerze DNS, najmniej efektywne s± algorytmy nie wykorzystuj±ce ¿adnych 
dodatkowych informacji, czyli Random i Round--robin.

Wykorzystuj±c informacje o kliencie, np. jego adres IP, mo¿na zastosowaæ bardziej efektywne rozwi±zania. Przyk³adem takiego 
algorytmu jest Client partition.

W zwi±zku z zarz±dzaniem strumieniem zapytañ na poziomie pakietów, odwo³ania nadchodz±ce z tego samego adresu, dotycz±ce 
pojedynczej sesji, musz± byæ przekierowywane do tego samego serwera. Dopiero ¿±dania nowego obiektu mog± byæ przes³ane na inny 
serwer. Wymaga to utrzymywania w systemie tablicy aktywnych po³±czeñ. Dziêki temu mo¿na oszacowaæ, który z serwerów obci±¿ony 
bêdzie dodatkowymi zapytaniami w najbli¿szym czasie i na tej podstawie wybraæ najlepszy, do którego skierowane zostanie 
kolejne zapytanie. Realizowane jest to na podstawie algorytmu Least loaded server. 
\begin{figure}[h]
\centering
\includegraphics[width=4.9in]{./rysunki/level_4_alg.eps}
\caption{Algorytmy stosowane w dystrybutorach.}
\label{level_4_alg}
\end{figure}

Oprócz informacji o tym, kto przesy³a zapytania do systemu webowego, mo¿na równie¿ uzyskaæ dane na temat pracy poszczególnych 
serwerów w systemie. Aby poprawiæ jako¶æ szeregowania zapytañ, w algorytmie dokonuj±cym wyboru najlepszego serwera stosuje siê 
ró¿nego rodzaju metryki okre¶laj±ce aktualne obci±¿enie serwerów. Najczê¶ciej stosowanymi algorytmami w tym przypadku s± Least 
loaded server oraz Weighted Round--robin. 
\begin{itemize}
\item W przypadku algorytmu Least loaded server dziêki wybranemu kryterium (okre¶lonej metryce) wiemy, który z serwerów 
powinien przyj±æ kolejne zapytanie. Odbywa siê to na zasadzie: najmniej obci±¿ony serwer przyjmuje kolejne zlecenie. 
\item Stosuj±c algorytm Weighted Round--robin, przy wyborze kolejnego serwera mo¿na uwzglêdniaæ metrykê jako parametr funkcji 
wyboru najlepszego serwera.

Wa¿nym wiêc elementem funkcjonowania tego rozwi±zania jest dobór metryki bêd±cej powy¿szym parametrem. Istnieje 
mo¿liwo¶æ kontrolowania nastêpuj±cych parametrów:
\begin{itemize}
\item input metric -- informacja pozyskiwana jest przez dystrybutor bez wspó³prcy z serwerami, np. liczba aktywnych po³±czeñ 
miedzy dystrybutorem a poszczególnymi serwerami,
\item server metric -- informacja pozyskiwana jest przez serwery i dostarczana dystrybutorowi, np. wykorzystanie procesora lub 
dysku, czas miedzy otrzymaniem zapytania a wys³aniem odpowiedzi,
\item forward metric -- informacja pozyskiwana jest bezpo¶rednio przez dystrybutor, np. emulacja zapytañ do serwerów webowych.
\end{itemize}
\end{itemize}

\subsection{Algorytmy szeregowania wykorzystywane w prze³±cznikach webowych}
Prze³±czniki webowe maj± równie¿ mo¿liwo¶æ kontrolowania 100\% ruchu do serwerów. Zatem i tu konieczne jest wykorzystanie jak 
najprostszych mechanizmów zarz±dzania. W przypadku prostych rozwi±zañ wystarczaj±co wydajne s± algorytmy statyczne. Sytuacja 
taka wystêpuje, gdy czasy realizacji zleceñ przez serwery WWW s± bardzo zbli¿one do siebie i nie wychodz± poza okre¶lony 
przedzia³ warto¶ci.

W przypadku gdy w systemie wystêpuj± wiêcej ni¿ dwa ograniczenia czasu obs³ugi zlecenia, nale¿y stosowaæ algorytmy dynamiczne 
wykorzystuj±ce informacje o kliencie lub stanie serwera (client info or server state aware). Maj±c do dyspozycji 
heterogeniczne ¶rodowisko serwerów trudno jest wybraæ najlepszy, wspólny dla wszystkich parametr okre¶laj±cy metrykê 
obci±¿enia serwera. W takim przypadku preferowane s± algorytmy wykorzystuj±ce informacje pochodz±ce od klientów.
\begin{figure}[h]
\centering
\includegraphics[width=4.9in]{./rysunki/level_7_alg.eps}
\caption{Algorytmy stosowane w prze³±cznikach webowych.}
\label{level_7_alg}
\end{figure}

Jak wynika z rysunku \ref{level_7_alg}, istniej± trzy rodzaje algorytmów opartych na informacjach o kliencie:
\begin{itemize}
\item Session identifiers -- odwo³ania HTTP, maj±ce ten sam identyfikator SSL lub ten sam znacznik cookie przypisywane s± do 
tego samego serwera -- zmniejsza to czas konieczny na ponown± identyfikacjê klienta,
\item Content partition -- podzia³ zasobów serwerów mo¿e nast±piæ ze wzglêdu na:
\begin{itemize}
\item typy plików -- dane, pliki graficzne, pliki audio umieszczone s± na specjalizowanych serwerach,
\item wielko¶æ plików -- du¿e pliki na szybszych serwerach lub równomierne roz³o¿enie plików w przypadku serwerów 
homogenicznych, 
\end{itemize}
\item Multi--class round--robin -- zasoby s± podzielone ze wzglêdu na z³o¿ono¶æ obliczeniow± i czasow±, jaka zostanie 
wygenerowana podczas ich obs³ugi, np. po³±czenia szyfrowane wymagaj± mocy obliczeniowej procesora, odwo³ania do bazy danych 
wymagaj± zwiêkszonej obs³ugi dysków, czy wreszcie pobieranie du¿ych plików w znacznym stopniu obci±¿a sieæ.
\end{itemize}

Zasada dzia³ania algorytmu wykorzystuj±cego informacje o kliencie i serwerze jest nastêpuj±ca. Pierwsze zapytanie klienta o 
zasoby przekierowywane jest wed³ug algorytmu Least loaded server (metryk± jest ilo¶æ aktywnych po³±czeñ z serwerem). Pozosta³e 
zapytania klienta o ten sam zasób przekierowywane s± do tego samego serwera. Dziêki temu zwiêkszona jest skuteczno¶æ odwo³añ 
do pamiêci podrêcznej (cache) tego serwera. Algorytm ten zwany jest Locality-Aware Request Distribution (LARD).

\subsection{Algorytmy szeregowania wykorzystywane w przypadku przekierowañ na poziomie protoko³u HTTP}

Gdy stosuje siê rozwi±zania oparte na warstwowej strukturze, istnieje mo¿liwo¶æ zarz±dzania zapytaniami z poziomu protoko³u \cite{modele18}. 
Takie rozwi±zanie jest przezroczyste dla u¿ytkowników. G³ównym celem stosowania tego mechanizmu jest zapobieganie 
przeci±¿eniom serwerów webowych. Przekierowywanie odbywa siê poprzez przes³anie klientowi informacji w nag³ówku: HTTP OK. 
302 -- Moved temporary to a new location.

Adres nowej lokalizacji mo¿e byæ podany w postaci nazwy domenowej lub adresu IP. Podanie adresu IP jest bardziej efektywne, 
poniewa¿ nastêpuje bezpo¶rednie odwo³anie do nowego serwera (klastra), a nie do serwera DNS.
Przekierowania mo¿na realizowaæ w zale¿no¶ci od kilku parametrów \cite{modele13}. Proces przekierowañ mo¿e dotyczyæ:
\begin{itemize}
\item wszystkich stron,
\item tylko stron przekraczaj±cych okre¶lon± wielko¶æ,
\item tylko stron, których ilo¶æ obiektów sk³adowych przekracza okre¶lon± liczbê,
\end{itemize}

Wybór serwera, który powinien przej±æ zapytanie, mo¿e odbywaæ siê z wykorzystaniem jednej z poni¿szych strategii:
\begin{itemize}
\item Round--robin,
\item Least Loaded,
\item Hash function,
\item Client to server proximity.
\end{itemize}

\section{Metody równowa¿enia obci±¿eñ -- przyk³ady}

\subsubsection{Równowa¿enie obci±¿eñ z wykorzystaniem filtra datagramów}

Inn± implementacj± rozproszonego algorytmu wspó³dzielenia obci±¿eñ jest zastosowanie na ka¿dym serwerze 
wchodz±cym w sk³ad klastra tzw. filtra datagramów. Klaster taki powinien byæ po³±czony z Internetem poprzez 
pojedynczy router brzegowy. Ka¿dy serwer w klastrze posiada skonfigurowane dwa adresy IP: adres ,,prywatny'' i 
jednakowy dla wszystkich serwerów adres IP klastra. Router po otrzymaniu datagramu opatrzonego adresem IP 
klastra nadaje mu fizyczny (sprzêtowy np. adres Ethernet) adres typu broadcast (je¿eli do routera przy³±czone 
s± tylko serwery nale¿±ce do klastra) lub multicast (je¿eli klaster jest tylko wyró¿nion± grup± w¶ród 
wszystkich hostów przy³±czonych do serwera). Zastosowanie takiego adresu sprawia, ¿e karty sieciowe wszystkich 
serwerów w klastrze akceptuj± datagram. Pomiêdzy sterownikiem karty sieciowej, a oprogramowaniem TCP/IP na 
ka¿dym serwerze musi pracowaæ specjalny proces, który zadecyduje, czy pakiet nale¿y odrzuciæ czy obs³u¿yæ. 
Proces ten nazywamy filtrem pakietów \cite{barylo34}. Decyzja o odrzuceniu lub obs³u¿eniu datagramu podejmowana jest na 
podstawie zawarto¶ci dwóch struktur danych: tablicy po³±czeñ TCP (datagramy nale¿±ce do jednego po³±czenia 
TCP musz± byæ obs³ugiwane przez serwer który nawi±za³ dane po³±czenie) oraz tablicy zawieraj±cych wielko¶æ 
obci±¿enia poszczególnych serwerów klastra. Tablica ta jest uaktualniana przez same serwery. Ka¿dy serwer musi 
wysy³aæ okresowo komunikat typu broadcast zawieraj±cy wielko¶æ jego obci±¿enia. Je¿eli do klastra nadchodzi 
datagram otwieraj±cy nowe po³±czenie TCP (nag³ówek TCP zawiera flagê SYN) serwer o najni¿szym indeksie 
obci±¿enia w tablicy (indeksem tym jest zwykle liczba otwartych po³±czeñ TCP) jest zobowi±zany do jego 
przyjêcia. Dla zwiêkszenia wydajno¶ci klastra w serwerach stosowaæ mo¿na dwie karty sieciowe: jedn± ze 
skonfigurowanym adresem IP klastra i drug± z prywatnym adresem serwera.  W takim przypadku pakiety, które nie 
musz± byæ filtrowane (np. wymiana danych SNMP) przechodziæ bêd± przez  ,,prywatn±'' kartê. Ten typ równowa¿enia 
dotyczyæ mo¿e ka¿dej us³ugi korzystaj±cej z TCP, w szczególno¶ci WWW.
Pierwsz± komercyjn± implementacj± tego rozwi±zania by³ pakiet oprogramowania Convoy Cluster firmy 
Valence Research przeznaczony dla systemu Microsoft Windows NT. Jako miarê obci±¿enia serwera przyjêto w nim 
ilo¶æ otwartych po³±czeñ TCP, a komunikaty og³aszaj±ce stan obci±¿enia wysy³ane by³y przez serwery co sekundê. 
Oprogramowanie umo¿liwia³o dynamiczn± konfiguracjê klastra przez dodawanie lub usuwanie serwera z klastra bez 
przerywania pracy klastra. Serwery w klastrze musia³y powielaæ swoje dane. W pierwszej wersji wymagane by³o 
stosowanie dwóch kart sieciowych na ka¿dym serwerze, a nadchodz±ce datagramy rozg³aszane by³y w trybie broadcast 
(dociera³y do ka¿dego hosta w sieci lokalnej klastra, nawet je¶li nie by³ on serwerem) Wersja 2.0 Convoy 
Cluster wyeliminowa³a te niedogodno¶ci i oferowa³a liczne dodatkowe mo¿liwo¶ci konfiguracyjne np. opcjonalne 
stosowanie pokrewieñstwa z klientem (ang. \emph{client affinity}), co umo¿liwia obs³ugê wszystkich datagramów 
nadchodz±cych od raz zidentyfikowanego (w trakcie nawi±zywania pierwszego po³±czenia) klienta przez jeden 
serwer. W roku 1999 firma Microsoft wykupi³a od Valence Research technologiê Convoy Cluster i po 
,,kosmetycznych'' zmianach udostêpni³a j± w pakiecie Microsoft Windows NT 4.0 Enterprise Server pod nazw± 
Microsoft Load Balancing Service.
Najwa¿niejsz± zalet± stosowania filtra pakietów jest jego du¿a wydajno¶æ w porównaniu do 
scentralizowanych urz±dzeñ rozdzielaj±cych zadania (np. LSNAT). Uzyskiwane jest to dziêki temu, ¿e na ¿adnym 
etapie obs³ugi zadania nie s± modyfikowane datagramy i nie istnieje pojedynczy punkt podejmowania decyzji o 
obs³udze zadania. Wa¿na jest równie¿ ³atwo¶æ konfiguracji klastra i du¿a niezawodno¶æ (serwer, który ulega 
awarii przestaje wysy³aæ komunikaty o stanie swego obci±¿enia, nie jest wiêc uwzglêdniany w tablicach obci±¿enia 
serwerów w pozosta³ych wêz³ach klastra i w ten sposób nie bierze udzia³u w równowa¿eniu obci±¿eñ). Koszt jaki 
nale¿y ponie¶æ, aby uzyskaæ te niew±tpliwie po¿±dane cechy to trudniejsza konfiguracja poszczególnych serwerów 
(konieczno¶æ instalacji i konfiguracji filtra pakietów) oraz du¿y ruch w sieci lokalnej klastra wynikaj±cy z 
aktualizacji tablic obci±¿enia. Aktualizacje te musz± byæ czêste, gdy¿ ³atwo mo¿na wyobraziæ sobie sytuacjê, w 
której serwer o najni¿szym indeksie obci±¿enia ulega awarii. W takiej sytuacji pozosta³e serwery a¿ do 
aktualizacji swoich tablic obci±¿enia bêd± odrzucaæ wszystkie datagramy otwieraj±ce nowe po³±czenia, co z 
pewno¶ci± nie jest zjawiskiem po¿±danym.

\subsubsection{Równowa¿enie obci±¿eñ z wykorzystaniem redirekcji HTTP}

Redirekcja jest  mechanizmem protoko³u HTTP, który zaprojektowano z my¶l± o obs³udze sytuacji w których 
zasób (plik) wskazywany przez pewien URL zmienia swoje fizyczne po³o¿enie (zostaje przeniesiony na inny serwer) 
i w zwi±zku z tym uzyskuje inny URL. Aby nie zmuszaæ u¿ytkownika do poszukiwania tego zasobu na w³asn± rêkê 
serwer WWW przechowuje tzw. tablice redirekcji. Jest to tablica zawieraj±ca URL, które wcze¶niej dotyczy³y 
zasobów danego serwera, lecz obecnie zasoby te znajduj± siê pod innym URL. Tablica zawiera równie¿ aktualny URL 
dla ka¿dego przeniesionego zasobu. W przypadku zapytania o taki ,,zdezaktualizowany'' URL serwer WWW zwraca 
odpowied¼ HTTP typu ,,przeniesiono'' i jako dane przekazuje aktualny URL zasobu. Przegl±darka po otrzymaniu takiej 
odpowiedzi musi zestawiæ nowe po³±czenie z serwerem wskazanym przez otrzymany URL.

Opisany mechanizm w prosty sposób wykorzystaæ mo¿na do pewnego rodzaju równowa¿enia obci±¿eñ serwerów 
WWW \cite{barylo22,barylo23}. W klastrze serwerów wydzieliæ mo¿na tzw. serwer redirekcji, którego nazwa DNS reprezentowaæ 
bêdzie ca³y klaster. Jedynym zadaniem takiego serwera jest utrzymywanie tablicy redirekcji i przekierowanie 
nadchodz±cych zapytañ do odpowiedniego serwera z klastra. W takiej architekturze serwery zwykle nie powielaj± 
swych zasobów, ka¿dy z nich przechowuje czê¶æ danych udostêpnianych przez klaster, a to który z nich obs³u¿y 
zapytanie determinowane jest przez rodzaj ¿±danych informacji. Przyk³adowo je¶li pod nazw± www.pogoda.com 
dostêpny jest serwis prezentuj±cy prognozê pogody dla ka¿dego kontynentu to zasoby serwisu podzieliæ mo¿na 
pomiêdzy siedem serwerów (po jednym dla ka¿dego kontynentu) a pod adresem IP stanowi±cym odwzorowanie nazwy 
serwisu nale¿y umie¶ciæ serwer redirekcji. W odpowiedzi na zapytanie o dowolny URL zaczynaj±cy siê np. od ci±gu 
www.pogoda.com/Azja/ serwer redirekcji dokonywa³by przekierowania do serwera przechowuj±cego dokumenty o 
pogodzie w Azji (np. wwwazja.pogoda.com) \cite{barylo22}. Poni¿szy rysunek przedstawia schemat nawi±zywania 
po³±czenia w przypadku stosowania redirekcji HTTP:
\begin{figure}[h]
\centering
\includegraphics[width=4.9in]{./rysunki/redirekcja.eps}
\caption{Schemat redirekcji HTTP.}
\label{redirekcja}
\end{figure}

Stosowanie redirekcji ma zasadniczo dwie zalety. Po pierwsze utrzymanie statycznej tablicy redirekcji jest 
bardzo proste i tanie w implementacji, nie wymaga stosowania specjalnego sprzêtu ani oprogramowania. Po drugie, 
poniewa¿ u¿ywane s± adresy URL, klaster stanowi±cy logiczn± ca³o¶æ mo¿e byæ rozproszony geograficznie tzn. 
serwer prezentuj±cy dane o pogodzie w Azji mo¿e rzeczywi¶cie znajdowaæ siê na terenie tego kontynentu, co mo¿e 
byæ dobrym pomys³em przy za³o¿eniu, ¿e o pogodê w Azji pytaæ bêd± g³ównie Azjaci.
Redirekcja ma jednak wiele wad \cite{barylo30}. Przede wszystkim ograniczona jest do protoko³u HTTP, a jak wiadomo 
wiele ³±cz hipertekstowych dokonuje prze³±czenia do np. serwerów FTP, które czêsto pracuj± na fizycznie tych 
samych komputerach, co serwery WWW. Druga wada jest wyra¼nie widoczna na Rys. \ref{redirekcja}. Aby rozpocz±æ pobieranie 
¿±danego dokumentu przegl±darka musi dokonaæ dwóch po³±czeñ, najpierw z serwerem redirekcji, a nastêpnie z 
w³a¶ciwym serwerem. Wprowadza to znaczne opó¼nienie i powoduje dodatkowe obci±¿enie sieci, która jest czêsto 
w±skim gard³em wydajno¶ci WWW. 

Prezentowane powy¿ej podej¶cie jest z gruntu statyczne i zak³ada wiedzê o tym, które dane bêd± 
poszukiwane najczê¶ciej, co umo¿liwia aprioryczne przydzielenie najsilniejszego serwera w klastrze do obs³ugi 
najpopularniejszej czê¶ci serwisu WWW. Poniewa¿ tablica redirekcji nie zawiera ¿adnych danych o obci±¿eniu 
poszczególnych serwerów trudno jest  dynamicznie uwzglêdniaæ takie dane podczas wyboru serwera. W literaturze 
proponowano architektury piêtrowe. Przyk³adowo dane dotycz±ce pogody w Azji mog³yby byæ powielane pomiêdzy 
kilka serwerów, które raportowa³yby stopieñ swego obci±¿enia, a na podstawie tych danych serwer redirekcji 
móg³by dynamicznie aktualizowaæ tablice redirekcji. Zbyt czêsta aktualizacja tej tablicy czyni j± jednak 
bezu¿yteczn± (tablica jest niedostêpna w trakcie aktualizacji), a zbyt rzadka powoduje nierównomierno¶æ 
obci±¿enia. Innym rozwi±zaniem jest zachowanie podzia³u na serwery tematyczne z mo¿liwo¶ci± dynamicznego 
przeniesienia czê¶ci zawarto¶ci z serwera mocno obci±¿onego na serwer posiadaj±cy rezerwê wydajno¶ci. 
Aktualizacje tablicy redirekcji by³yby wtedy rzadsze, lecz procedura taka wymaga³aby kosztownego ¶ledzenia, 
które pliki pobierane s± najczê¶ciej (tylko przeniesienie takich plików znacz±co mo¿e zmniejszyæ obci±¿enie 
serwera), dodatkowo dane by³yby niedostêpne przez czas przenoszenia. Obydwie metody dynamicznego wykorzystania 
tablicy redirekcji mog± byæ ominiête przez u¿ytkownika, je¶li po po³±czeniu z serwerem docelowym umie¶ci on 
zak³adkê (ang. \emph{bookmark}) na pobieranych stronach WWW. 
Statyczna redirekcja HTTP jest skuteczna tylko w przypadku serwisów charakteryzuj±cych siê ³atwym do 
przewidzenia wzorcem dostêpu do dokumentów.

\subsubsection{Równowa¿enie obci±¿eñ z wykorzystaniem NAT}

Mechanizm translacji adresów sieciowych NAT (ang. \emph{Network Address Translation}) zosta³ zaprojektowany z 
my¶l± o mo¿liwo¶ci w³±czenia prywatnych sieci lokalnych do Internetu z wykorzystaniem jednego, globalnie 
unikalnego adresu IP dla ca³ej sieci. W standardowej konfiguracji urz±dzeniem realizuj±cym NAT jest router 
brzegowy o adresie IP reprezentuj±cym ca³± sieæ, który stanowi jedyne po³±czenie pomiêdzy sieci± prywatn± a 
rozleg³±. Hosty w sieci prywatnej nie musz± posiadaæ globalnie unikalnych adresów IP, gdy¿ podczas nawi±zywania 
po³±czenia z komputerem spoza sieci prywatnej urz±dzenie NAT zamienia adres nadawcy datagramu IP (pochodz±cy z 
sieci prywatnej) na swój w³asny, dokonuje przeliczenia odpowiednich sum kontrolnych i aby poprawnie kierowaæ 
datagramami nale¿±cymi do jednego po³±czenia TCP zapamiêtuje w wewnêtrznych tablicach parametry po³±czenia 
(adresy i porty ¼ród³owe i docelowe). Celem wprowadzenia mechanizmu NAT by³o zapewnienie pewnego stopnia 
bezpieczeñstwa sieciom prywatnym, gdy¿ je¶li wewn±trz takiej sieci komputery nie posiadaj± globalnie unikalnych 
adresów IP, to nie istnieje mo¿liwo¶æ  nawi±zania po³±czenia z takim komputerem spoza sieci prywatnej. 
Istnieje wiele rozwi±zañ komercyjnych realizuj±cych równowa¿enie (wspó³dzielenie)  obci±¿eñ serwerów 
WWW poprzez mechanizm NAT. Idea polega na kierowaniu zapytañ nadchodz±cych do klastra serwerów poprzez 
specjalizowane urz±dzenie NAT, okre¶lane czasem jako LSNAT (ang. \emph{Load Sharing NAT}), które kierowa³oby zapytanie 
do konkretnego serwera \cite{barylo7}. Algorytm wed³ug którego zapytanie by³yby kierowane do serwerów mo¿e uwzglêdniaæ 
ró¿nice w ich wydajno¶ci jak i stopieñ ich obci±¿enia, istnieje równie¿ mo¿liwo¶æ uwzglêdnienia w nim numeru 
portu TCP, co sprawia, ¿e LSNAT stosowaæ mo¿na do równowa¿enia obci±¿eñ wszystkich us³ug TCP. Obci±¿enie 
serwerów okre¶lane jest zazwyczaj na podstawie tablicy otwartych po³±czeñ utrzymywanej przez LSNAT dla ka¿dego 
serwera. Aby dane te by³y aktualne konieczna jest analiza nag³ówków TCP w celu wykrywania datagramów 
zamykaj±cych po³±czenie. Serwery w klastrze powinny powielaæ swoje dane, gdy¿ wykorzystanie rozproszonego 
systemu plików powoduje zbyt du¿e obci±¿enie sieci lokalnej klastra. Poni¿ej schematycznie przedstawiono dwie 
typowe konfiguracje urz±dzenia NAT jako modu³u realizuj±cego  równowa¿enie obci±¿eñ. Na ka¿dym rysunku klaster 
serwerów reprezentowany jest przez adres IP 172.87.0.100.

W takiej konfiguracji w datagramach przychodz±cych do klastra nastêpuje zmiana adresu docelowego z adresu 
urz±dzenia LSNAT na adres wybranego serwera oraz zmiana adresu nadawcy na adres urz±dzenia LSNAT. W datagramach 
wysy³anych w przeciwnym kierunku adres nadawcy zmienia siê z adresu serwera na adres LSNAT a adres docelowy z 
adresu LSNAT na adres rzeczywistego odbiorcy datagramu. Takie postêpowanie powoduje, ¿e wszystkie datagramy 
kierowane do klastra i z powrotem musz± przej¶æ przez urz±dzenie LSNAT, co umo¿liwia skonfigurowanie klastra 
rozproszonego geograficznie. Dzieje siê tak kosztem utrzymywania w LSNAT bardziej rozbudowanej (w stosunku do 
poprzedniej konfiguracji) tablicy po³±czeñ, która umo¿liwia³aby identyfikacjê ka¿dego po³±czenia. Identyfikacji 
tej dokonuje siê wykorzystuj±c numery portów TCP (wraz z translacj± adresów datagramu dokonuje siê zmiany 
numeru portu ¼ród³owego na unikaln± dla danego serwera warto¶æ powy¿ej 5000, identyfikacji odpowiedzi adresata
dokonuje siê na podstawie adresu serwera, który j± wys³a³ i numeru portu docelowego). Powy¿sza konfiguracja 
pracowaæ mo¿e równie¿ z adresami prywatnymi, uniemo¿liwia to jednak u¿ycie serwerów odleg³ych geograficznie.
Poni¿ej przedstawiono krótki opis dwóch popularnych rozwi±zañ komercyjnych wykorzystuj±cych mechanizm 
NAT do równowa¿enia obci±¿eñ serwerów WWW.

Rozwi±zania korzystaj±ce z mechanizmu NAT do równowa¿enia obci±¿eñ s± znacznym postêpem w stosunku do metod 
opisanych wcze¶niej w tej pacy. Umo¿liwiaj± skuteczne uwzglêdnienie stopnia obci±¿enia poszczególnych serwerów 
w klastrze jak i ich indywidualnych w³a¶ciwo¶ci. LSNAT umo¿liwia rozró¿nianie po³±czeñ na podstawie numerów 
portów TCP jak i równowa¿enie obci±¿eñ pomiêdzy serwery odleg³e geograficznie. Metoda ta nie jest jednak 
pozbawiona wad. W oczywisty sposób urz±dzenie LSNAT staje siê w±skim gard³em wydajno¶ci klastra, gdy¿ ca³y 
ruch pomiêdzy klasterem, a Internetem musi przez nie przechodziæ. Je¶li wzi±æ pod uwagê, ¿e w przypadku WWW 
objêto¶æ odpowiedzi serwera jest co najmniej dziesiêciokrotnie wiêksza ni¿ zapytanie, jasnym staje siê, ¿e w 
obliczu ci±g³ego wzrostu liczby u¿ytkowników, najwydajniejsze nawet urz±dzenie LSNAT mo¿e w koñcu ograniczyæ 
wydajno¶æ klastra. Nale¿y równie¿ zauwa¿yæ, ¿e zmiana adresu IP w nag³ówku datagramu poci±ga za sob± 
konieczno¶æ wyliczenia nowej sumy kontrolnej dla ca³ego datagramu. Jest to operacja czasoch³onna przez co 
wprowadza opó¼nienie w transmisji danych jak i konieczno¶æ kolejkowania pakietów w samym urz±dzeniu (trudno 
oczekiwaæ, ¿e nawet urz±dzenie przetwarzaj±ce klika pakietów równolegle poradzi sobie bez opó¼nieñ z ca³ym 
przechodz±cym przez nie ruchem). Ta w³a¶ciwo¶æ LSNAT znacznie ogranicza jego skalowalno¶æ, gdy¿ dodawanie 
nowych serwerów do klastra w prosty sposób zwiêksza kolejkê pakietów w urz±dzeniu, a¿ do momentu, w którym 
przekroczone zostan± jego mo¿liwo¶ci lub cierpliwo¶æ u¿ytkowników. 

\subsubsection{Równowa¿enie obci±¿eñ poprzez ,,pó³--po³±czeniowe'' marszrutowanie TCP}

Rozpatruj±c przedstawione kolejno w tej pracy metody równowa¿enia obci±¿eñ mo¿na zauwa¿yæ pewn± 
prawid³owo¶æ. Otó¿ im dana metoda jest bardziej skuteczna i zaawansowana koncepcyjnie, tym w ni¿szej warstwie 
sieciowej operuje. Redirekcja HTTP i RR--DNS dzia³a³y powy¿ej warstwy transportowej, w ogóle nie ingeruj±c w 
zawarto¶æ datagramów IP. Rozwi±zania oparte o LSNAT i DPR pracowa³y w warstwie sieciowej i aby skutecznie 
rozdzielaæ zadania pomiêdzy serwery musia³y dokonywaæ modyfikacji (adresów i sum kontrolnych) w nag³ówkach  
datagramów IP. Metoda opisana w tym punkcie operuje w warstwie fizycznej  i do rozdzia³u zadañ nie musi 
zmieniaæ zawarto¶ci datagramów IP.

,,Pó³--po³±czeniowe'' marszrutowanie TCP (ang. \emph{half--connection TCP routing}) jest opatentowan± przez 
firmê IBM technologi±, która leg³a u podstaw zasady dzia³ania pakietu oprogramowania Network Dispatcher \cite{barylo35,barylo36}. 
W swej podstawowej konfiguracji pakiet ten umo¿liwia zestawienie klastra z³o¿onego z po³±czonych 
sieci± lokaln± serwerów korzystaj±cych TCP lub UDP (w szczególno¶ci serwerów WWW) i udostêpnienie go pod 
jednym adresem IP. W klastrze tym serwery maj± unikalne globalnie lub lokalnie adresy IP i  musz± powielaæ 
swoje dane. W obszarze tej samej sieci lokalnej, w której dzia³a klaster, musi byæ wyznaczony komputer, na 
którym pracowaæ bêdzie oprogramowanie Network Dispatcher. Komputer ten musi posiadaæ dwa adresy IP, jeden z 
nich, tzw. adres NFA (ang. \emph{Non--Forwarding Address}) jest ,,osobistym'' adresem komputera, pod którym mo¿na 
skontaktowaæ siê z pracuj±cym na tym komputerze oprogramowaniem (np. z agentem SNMP). Drugi adres, to adres 
reprezentuj±cy klaster serwerów, wszystkie datagramy IP opatrzone tym adresem bêd± przetwarzane przez program 
Network Dispatcher i na podstawie algorytmu wspó³dzielenia obci±¿eñ przekazywane do jednego z serwerów w 
klastrze. Dispatcher zmienia jedynie docelowy adres fizyczny (sprzêtowy) datagramu, zawarty w nag³ówku 
sprzêtowym, dodawanym przed nag³ówek IP (np. w nag³ówku Ethernet). Dziêki temu datagramy wysy³ane przez serwer 
w odpowiedzi mog± byæ kierowane bezpo¶rednio do klienta, bez konieczno¶ci ponownego przej¶cia przez 
oprogramowanie Network Dispatcher (w celu np. przywrócenia oryginalnych adresów IP). St±d pochodzi nazwa tej 
metody -- ,,pó³--po³±czeniowe'' marszrutowanie TCP. Serwer wysy³aj±c odpowied¼ dokonuje standardowej zamiany adresów 
IP nadawcy i odbiorcy pobieraj±c obydwa te adresy z nag³ówka otrzymanego datagramu. Jak pamiêtamy adresem 
docelowym jest w tym datagramie adres klastra (komputera, na którym pracuje Dispatcher), wiêc adres ten stanie 
siê adresem nadawcy odpowiedzi, co sprawi, ¿e kolejne datagramy dotycz±ce danego po³±czenia skierowane zostan± 
na adres Network Dispatcher--a \cite{barylo36}.

Adres IP ka¿dego serwera w klastrze jest ró¿ny od adresu klastra. Fakt, ¿e pomimo to oprogramowanie 
TCP/IP w serwerze akceptuje pakiety opatrzone adresem klastra jest mo¿liwy dziêki dodaniu aliasu do adresu 
interfejsu pêtli zwrotnej (ang. \emph{loopback interface}) w ka¿dym serwerze. Do standardowego adresu 127.0.0.1 
dodawany jest jako alias adres klastra (w przypadku przedstawionym na rysunku 139.37.38.39). Mo¿liwo¶æ 
nadawania wielu adresów interfejsowi pêtli zwrotnej jest jedynym wymaganiem Dipatcher-a w stosunku do serwerów.
Poniewa¿ Network Dispatcher rozró¿nia porty TCP i UDP mo¿liwe jest równowa¿enie obci±¿eñ powodowanych 
przez dowolny protokó³ korzystaj±cy z TCP lub UDP m.in. HTTP (WWW), FTP, SSL, SMTP, POP3 czy Telnet. Poniewa¿ 
wszystkie serwery powielaj± swoje dane, mo¿liwa jest sytuacja w której np. plik HTML opisuj±cy stronê WWW 
pobierany jest z serwera A, a pliki graficzne sk³adaj±ce siê na stronê pobierane s± z serwera B.
W celu zarz±dzania po³±czeniami Network Dispatcher przechowuje dwie struktury danych- tablicê po³±czeñ 
aktywnych (otwartych) i tablicê nowo przydzielonych po³±czeñ. Tablica otwartych po³±czeñ s³u¿y do poprawnego 
kierowania datagramów dotycz±cych nawi±zanego po³±czenia TCP (po³±czenie TCP nie mo¿e byæ przez Dipatcher 
przekazane pomiêdzy serwerami). Zawiera ono adres IP nadawcy i numer ¼ród³owego portu TCP oraz adres IP serwera, 
który obs³uguje po³±czenie i numer portu docelowego TCP oraz pole stanu po³±czenia. Pozycje z tej tablicy s± 
usuwane po wykryciu w nag³ówku TCP flagi FIN lub RST. Ilo¶æ po³±czeñ otwartych na danym serwerze jest równie¿ 
uwzglêdniana podczas obliczenia wagi serwera. Tablica nowo przydzielonych po³±czeñ s³u¿y do zapamiêtania jak 
wiele po³±czeñ zosta³o przydzielonych do danego serwera od ostatniego od¶wie¿enia wag serwerów i jako miara 
prêdko¶ci zmian obci±¿enia serwera wykorzystywana jest do obliczenia jego wagi.

Je¿eli na adres Dispatcher--a przychodzi datagram nie nale¿±cy do ¿adnego z po³±czeñ zapisanych w 
tablicy aktywnych po³±czeñ, oznacza to, ¿e jest to nowe po³±czenie i nale¿y przydzieliæ serwer do jego obs³ugi. 
Dispatcher utrzymuje cykliczn± listê serwerów zawieraj±c± ich wagi. Waga serwera oddaje stopieñ jego obci±¿enia 
i jest obliczana przez Dispatcher okresowo dla klastra (dla wszystkich serwerów jednocze¶nie). Dispatcher 
pamiêta numer i wagê serwera do którego przydzielono ostatnie po³±czenie TCP i rozpoczynaj±c przeszukiwanie 
listy od tego miejsca poszukuje serwera o wadze wiêkszej lub równej zapamiêtanej wadze (im wiêksza waga, tym 
mniej obci±¿ony serwer). Do znalezionego w ten sposób serwera przydzielane jest nowe po³±czenie, co znajduje 
odwzorowanie w tablicy aktywnych po³±czeñ. 

Waga dla ka¿dego serwera obliczana jest na podstawie ilo¶ci otwartych po³±czeñ TCP, ilo¶ci nowo 
przydzielonych po³±czeñ, stopnia obci±¿enia procesora w serwerze (miara ta uwzglêdnia równie¿ obci±¿enie 
wynikaj±ce z zadañ uruchamianych lokalnie i pochodzi od modu³u ISS (ang. \emph{Interactive Session Support}) pakietu 
Network Dispatcher, ISS uwzglêdnia równie¿ indywidualne w³a¶ciwo¶ci serwera) oraz na podstawie danych 
pochodz±cych z tzw. Advisor--ów. Advisor jest programem symuluj±cym klienta danego protoko³u i bada jak szybko 
serwer jest w stanie zareagowaæ na typowe dla danego protoko³u ¿±danie. Np. Advisor HTTP wysy³a do serwera 
¿±danie HTTP GET / (prze¶lij domy¶lny dokument g³ównego katalogu w serwisie) i jako wynik zwraca czas, po którym 
otrzyma³ pierwszy bajt odpowiedzi. Proporcje z jakimi poszczególne elementy wchodz± w sk³ad wagi, jak i 
czêstotliwo¶æ od¶wie¿ania wag ustalane s± przez administratora klastra.
Network Dispatcher posiada wiele cech wyró¿niaj±cych go spo¶ród przedstawionych wcze¶niej rozwi±zañ. 
Jest oczywi¶cie wolny od niekorzystnych cech RR--DNS i nie modyfikuje datagramów IP, a analiza, któr± na nich 
prowadzi (zapamiêtanie adresów i portów, wykrywanie flag), nie jest czasoch³onna. Dodatkowo ruch przechodz±cy 
przez Dispatcher--a stanowi± tylko datagramy nadchodz±ce do klastra (typowo mniejsze od odpowiedzi), dziêki 
czemu nie jest ³atwo (w przeciwieñstwie do urz±dzeñ LSNAT) tak obci±¿yæ Dispatcher--a by sta³ siê ,,w±skim 
gard³em'' wydajno¶ci klastra. W przeciwieñstwie do rozwi±zania z zastosowaniem DPR, Dispatcher nie wymaga 
specjalistycznego oprogramowania pracuj±cego na serwerze, wiêc nie obci±¿a go np. zadaniem przetwarzania 
datagramów IP.

Pierwszy prototyp Network Dispatcher--a obs³ugiwa³ Internetowy serwis IO w Atlancie w 1996. Kolejne, 
ju¿ komercyjne wersje obs³ugiwa³y takie wydarzenia jak turniej US Open i IO w Nagano w 1998 oraz mecze szachowe 
Deep Blue vs. Garri Kasparow. Szczytowe obci±¿enie klastra w przypadku IO w Nagano osi±gnê³o 110 414 zapytañ na
 minutê, a mimo to równowa¿enie obci±¿eñ przy u¿yciu Network Dispatcher--a zapewni³o wszystkim u¿ytkownikom dobry 
czas odpowiedzi i zadowalaj±cy transfer.

\subsubsection{Rówowa¿enie obci±¿eñ poprzez bezpo¶rednie routowanie -- \emph{Direct Routing}}

Architektura ta jest podobna do wykorzystywanej w produkcie firmy IBM -- oprogramowaniu SecureWay Network Dispatcher. 

Adres wirtualny serwera jest wspó³dzielony poprzez poszczególne nody i load balancer. Interfejs sieciowy dystrybutora jest 
skonfigurowany tak¿e do wirtualnego adresu, który jest wykorzystywany do przyjmowania pakietów przychodz±cych oraz do 
bezpo¶redniego routowania pakietów do wybranych serwerów. Wszystkie rzeczywiste serwery maj± swoje non--arp aliasy interfejsu 
sieciowego skonfigurowane z adresem wirtualnym lub bezpo¶rednio przekierowuj± pakiety przeznaczone na adres wirtualny do 
lokalnych gniazd, w ten sposób, ¿e rzeczywiste serwery mog± przenosiæ pakiety tylko lokalnie. Zarówno load balancer jak i 
rzeczywiste serwery musz± mieæ interfejsy sieciowe fizycznie skojarzone z HUB--em lub switchem. Wygl±da to w ten sposób, ¿e
dystrybutor po prostu zmienia adres MAC na adres rzeczywistego serwera i retransmituje do niego pakiet. 

\section{Przyk³ady produktów stosowanych do równowa¿enia obci±¿enia wielokomputerowych serwerów WWW}

\subsection{LinuxVirtualServer}

Linux Virtual Server jest wysoce skalowalnym i dostêpnym serwerem zbudowanym na klastrze rzeczywistych serwerów wraz 
z mo¿liwo¶ci± realizacji równowa¿enia obci±¿êñ. System ten oparty jest na systemie Linux. Architektura tego rozwi±zania
jest (jak i pozosta³e) przezroczysta dla klienta i odbywa siê na poziomie protoko³u IP (warstwa czwarta). 

Rzeczywiste serwery mog± byæ po³±czone w obrêbie sieci lokalnej lub geograficznie rozproszone w sieci WAN. Fron-endem tych
rzeczywistych serwerów jest dystrybutor (load balancer), który marszrutuje ¿±dania do ró¿nych serwerów oraz powoduje, ¿e
równoleg³e us³ugi dzia³aj±ce w obrêbie klastra wydaj± siê byæ jedn± wirtualn± us³ug± dla ca³ego klastra na jednym adresie IP.
Skalowalno¶æ w tym systemie oznacza, ¿e w przezroczysty sposób mo¿na dodawaæ i usuwaæ poszczególne nody do klastra. Wysoka 
dostêpno¶æ jest realizowana poprzez detekcjê uszkodzonych nodów lub niesprawnych demonów oraz równoczesn± rekonfiguracjê
systemu.

Virtual Server mo¿na implementowaæ na trzy sposoby:
\begin{description}
\item[Virtual Server poprzez NAT] -- zalet± tego rozwi±zania jest fakt, ¿e rzeczywiste serwery mog± pracowaæ na dowolnym
systemie operacyjnym, który w³ada protoko³em TCP/IP. Rzeczywiste serwery maj± prywatny adresy IP i tylko one s± potrzebne
dystrybutorowi do pracy. Wad± tego rozwi±zania jest raczej niewielka skalowalno¶æ, poniewa¿ w tym wypadku \emph{load balancer}
mo¿e stanowiæ w±skie gard³o ca³ego systemu gdy liczba pod³±czonych serwerów bêdzie wynosiæ oko³o 20 lub wiêcej. Jest to
spowodowane tym, ¿e zarówno ruch przychodz±cy (niewielki) jak i wychodz±cy (o wiele wiêkszy) s± przepisywane przez dystrybutora.
Mo¿na to omin±c poprzez korzystanie z pozosta³ych rozwi±zañ Virtual Servera lub poprzez rozwi±zanie hybrydowe z DNS i kilkoma
osobnymi Virtual Serverami;
\item[Virtual Server poprzez Tunelowanie IP] -- w tym wypadku load balancer tylko przekazuje
ruch wchodz±cy do poszczególnych rzeczywistych serwerów, za¶ one odpowiadaj± bezpo¶rednio do u¿ytkowników. W tym rozwi±zaniu 
jak widaæ serwer virtualny mo¿e siê sk³adaæ i z ponad 100 serwerów i nadal dystrybutor nie bêdzie stanowi³ w±skiego gard³a.
Maksymalna wydajno¶æ Virtual Servera w tym przypadku mo¿e siêgaæ powy¿ej 1Gbps -- w przypadku gdy dystrybutor dysponowaæ
bêdzie 100Mbps kart± sieciow±. Rozwi±zanie oparte na tunelowaniu IP mo¿e byæ u¿ywane do serwerów wirtualnych w bardzo 
wysokich wydajno¶ciach, szczególnie dobrych do tworzenia virtualnych proxy serwerów. Wad± tego rozwi±zania jest to, ¿e ka¿dy
serwer musi umieæ dokonywaæ enkapsulacji IP (tunelowanie IP) wewn±trz IP; 
\item[Virtual Server poprzez bezpo¶rednie routowanie]\footnote{ang. \emph{Direct Routing}} -- opisany powy¿ej. Wad± tego 
rozwi±zanie jest brak mo¿liwo¶ci rozbudowy wirtualnego serwera powy¿ej sieci lokalnej, jednak¿e w porównaniu z poprzedni±
architektur± rzeczywiste serwery nie potrzebuj± pos³ugiwaæ siê enkapsulacj± IP.
\end{description}

W po³±czeniu z ka¿d± implementacj± Virtual Server korzysta z nastêpuj±cych algorytmów dystrubuuj±cych pakiety:
\begin{itemize}
\item marszrutowanie algorytmem Round--Robin;
\item marszrutowanie algorytmem Weighted Round--Robin (statyczne wagi, bez wykorzystywania informacji o stanie systemów);
\item marszrutowanie typu Least Connection (dynamiczny algorytm -- opisany w tekscie);
\item marszrutowanie typu Weighted Least Connection (jak wy¿ej + statycznie nadawane wagi poszczególnym serwerom);
\item marszrutowanie Locality--Based Least Connection;
\item marszrutowanie Locality--Based Least Connection witch Replcation;
\item marszrutowanie typu Destination Hashing;
\item marszrutowanie typu Source Hashing.
\end{itemize}

Zalet± Virtual Servera jest jego dzia³anie w obrêbie j±dra co oznacza wysok± wydajno¶æ i stabilno¶æ. Kolejn± zalet± jest 
dostêpno¶æ kodu ¼ród³owego (oprogramowanie typu OpenSource) co oznacza mo¿liwo¶æ modyfikacji kodu w zale¿no¶ci od potrzeb.

\subsection{Cisco LocalDirector}

LocalDirector jest nazw± rodziny urz±dzeñ produkowanych przez firmê Cisco. Urz±dzenia te s³u¿± do 
równowa¿enia obci±¿eñ dowolnych serwerów wykorzystuj±cych TCP (np. serwery WWW, FTP, SSL) \cite{barylo30,barylo31}. 
LocalDirector instalowany jest w konfiguracji przedstawionej na Rys. \ref{LocalDirector} z wykorzystaniem adresów prywatnych, 
\begin{figure}[h]
\centering
\includegraphics[width=4in]{./rysunki/LocalDirector.eps}
\caption{LSNAT symetrycznie zmieniaj±cy adres IP.}
\label{LocalDirector}
\end{figure}

musi wiêc byæ instalowany jako jedyne po³±czenie pomiêdzy klasterem a bramk± (ang. \emph{gateway}) do sieci rozleg³ej. 
LocalDirector wymaga powielania zawarto¶ci pomiêdzy serwerami w klastrze. Serwery mog± byæ po³±czone z 
LocalDirector-em poprzez sieæ Ethernet, FastEthernet lub FDDI. Wybór serwera dokonywany jest sekwencyjnie na 
podstawie wag uwzglêdniaj±cych indywidualne w³a¶ciwo¶ci serwera i jego obci±¿enie w postaci liczby otwartych 
po³±czeñ TCP. LocalDirector sprawdza czy serwer siê nie za³ama³ poprzez okresowe nawi±zywanie z nim po³±czenia 
kontrolnego (Ping, HTTP GET /). Producent zapewnia, ¿e LocalDirector, w zale¿no¶ci od modelu, jest w stanie 
obs³u¿yæ od 7000 do 25000 po³±czeñ na sekundê przy przepustowo¶ci od 80 Mbit/sek. do 400 Mbit/sek. Istnieje 
mo¿liwo¶æ piêtrowej konfiguracji LocalDirector-a. Kilka rozproszonych geograficznie klasterów, z których ka¿dy 
obs³ugiwany jest przez jedno urz±dzenie LocalDirector, mo¿na zaprezentowaæ jako jeden klaster z u¿yciem tzw. 
GlobalDirectora. GlobalDirector to w rzeczywisto¶ci serwer DNS rozdzielaj±cy zapytania pomiêdzy klastery na 
zasadzie podobnej do RR--DNS. 

\subsection{F5 Labs BigIP}

BigIP firmy F5 Labs jest tzw. rozwi±zaniem ,,pod klucz'' (ang. \emph{turn--key solution}). Oznacza to, ¿e dostarczany 
jest tak jak urz±dzenie sprzêtowe (np. LocalDirector), choæ w rzeczywisto¶ci jest to komputer pracuj±cy pod 
kontrol± specjalizowanego oprogramowania i systemu operacyjnego firmy F5 Labs. BigIP umo¿liwia równowa¿enie 
obci±¿eñ ka¿dego serwera korzystaj±cego z protoko³u TCP lub UDP \cite{barylo32,barylo33}. Urz±dzenie mo¿e byæ pod³±czone w 
konfiguracji przedstawionej na Rys. \ref{LocalDirector} lub Rys. \ref{LocalDirector1} w obydwu jednak przypadkach, aby uniemo¿liwiæ ominiêcie 
\begin{figure}[h]
\centering
\includegraphics[width=4in]{./rysunki/LocalDirector1.eps}
\caption{LSNAT zmieniaj±cy jeden z adresów IP}
\label{LocalDirector1}
\end{figure}

mechanizmu równowa¿enia obci±¿eñ, BigIP powinien znajdowaæ siê pomiêdzy klasterem serwerów, a bramk± do 
Intetrnet-u. BigIP mo¿na po³±czyæ z klasterem poprzez sieæ Ethernet, FastEthernet, FDDI lub opcjonalnie 
GigaBitEthernet. Urz±dzenie oferuje do wyboru siedem algorytmów rozdzia³u zadañ. Trzy z nich to algorytmy 
statyczne, s± to: algorytm cykliczny (ang. \emph{Round--robin}); algorytm proporcjonalny przydzielaj±cy zadania wed³ug 
ustalonych na sta³e wag i algorytm priorytetowy, który umo¿liwia wydzielenie w klastrze grup serwerów o 
okre¶lonym priorytecie i rozdzia³ zadañ do grup, w ka¿dej z grup serwer do obs³ugi konkretnego zadania 
wskazywany jest cyklicznie. Pozosta³e algorytmy s± dynamiczne i uwzglêdniaj± obci±¿enie poszczególnych 
serwerów. S± to algorytm LC (ang. \emph{Least Connections}) przydzielaj±cy zadania do serwera utrzymuj±cego najmniej 
otwartych po³±czeñ, algorytm wyznaczaj±cy ten serwer, który najszybciej odpowie na zapytanie kontrolne (np. 
HTTP GET /), tzw. algorytm obserwacyjny bêd±cy kombinacj± powy¿szych i algorytm predyktywny, który przydziela 
zadania (po³±czenia) do serwera, którego obci±¿enie (mierzone jako kombinacja ilo¶ci otwartych po³±czeñ i czasu 
odpowiedzi na zapytanie kontrolne) zmniejsza³o siê najszybciej w ci±gu np. ostatnich 5 sekund.
Konkretne modele BigIP wyposa¿one s± w procesory Intel Pentium II 450 do 600 MHz pamiêæ RAM 128 MB do 
1GB i dysk twardy 4 GB do 8.4 GB. Producent gwarantuje przepustowo¶æ od 170 Mbit/sek. do 350 Mbit/sek. i 
obs³ugê do 20000 zapytañ na sekundê.
	
\subsection{IBM SecureWay Network Dispatcher}

W zwi±zku z tym, ¿e pakiet ten jest jednym z g³ównych elementów tej pracy, jego dok³adny opis i konfiguracja znajduje siê 
w nastêpnym rozdziale (Rozdz. \ref{r05}).

